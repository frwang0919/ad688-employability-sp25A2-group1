[
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\n\n\n\n\n\nSpecifically, we removed:\n- Older NAICS and SOC codes (e.g., NAICS2, SOC_2).\nThe North American Industry Classification System (NAICS) and Standard Occupational Classification (SOC) systems undergo periodic updates. Retaining only NAICS_2022_6 and SOC_2021_4 ensures we use the most recent classification standards. Moreover, older codes are redundant and may lead to inconsistencies in trend analysis.\n- Tracking data and URLs (e.g., ID, DUPLICATES).\nThese columns related to data collection timestamps, unique identifiers, or internal system references, which do not contribute to meaningful insights about the job market. Similarly, URLs are not necessary for our analysis as they do not provide any additional value or context but add unnecessary complexity to the dataset.\n\n\n\nThere are some reasons for this. Firstly, keeping only the latest industry and occupation classifications ensures our analysis reflects the most recent classification standards and avoid confusion and inconsistencies in classification. Additionally, reducing unnecessary columns speeds up data processing and enhances readability. This is particularly important when working with large datasets, as it minimizes the risk of errors and improves the efficiency of our analysis. Finally, it helps to focus on the most relevant information, allowing for clearer insights and conclusions regarding job market trends.\n\n\n\nBy removing outdated and irrelevant columns, we achieve:\n- More accurate job market trends, focusing on meaningful variables.\n- Easier interpretation without clutter from redundant or technical fields.\n- Faster analysis and visualization, improving overall efficiency.\n\njob_postings = pd.read_csv('lightcast_job_postings.csv')\n\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\", \"ACTIVE_URLS\", \"TITLE\", \"COMPANY\",\n    \"MSA\", \"STATE\", \"COUNTY\", \"CITY\", \"COUNTY_OUTGOING\", \"COUNTY_INCOMING\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    \"ONET\", \"ONET_2019\", \"CIP2\", \"CIP4\", \"CIP6\", \"MODELED_DURATION\", \"MODELED_EXPIRED\",\n    \"CERTIFICATIONS\", \"COMMON_SKILLS\", \"SPECIALIZED_SKILLS\", \"SKILLS\", \"SOFTWARE_SKILLS\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_OCCUPATION\", \"LOT_CAREER_AREA\",\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\",\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\", \"NAICS_2022_4\", \"NAICS_2022_4_NAME\",\n    \"NAICS_2022_5\", \"NAICS_2022_5_NAME\", \"NAICS_2022_6\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_5\", \"SOC_5_NAME\", \"SOC_4\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\", \"SOC_2021_4\"\n]\n\njob_postings.drop(columns = columns_to_drop, inplace = True)\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nmsno.heatmap(job_postings)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nThe SALARY column has a significant number of missing values. To handle this, we replaced the missing values with the median salary for that specific title or industry. This approach is effective because it minimizes the impact of outliers and provides a more accurate representation of the typical salary for each job title.\n\ntitle_median_salary = job_postings.groupby('TITLE_NAME')['SALARY'].median()\nindustry_median_salary = job_postings.groupby('NAICS_2022_6_NAME')['SALARY'].median()\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: title_median_salary[row['TITLE_NAME']]\n    if pd.isna(row['SALARY']) and row['TITLE_NAME'] in title_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: industry_median_salary[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['SALARY']) and row['NAICS_2022_6_NAME'] in industry_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'] = job_postings['SALARY'].fillna(job_postings[\"SALARY\"].median())\n\n\n\n\nDealing with columns that have more than 50% missing values is crucial for maintaining the integrity of our dataset. Columns with excessive missing data can introduce bias and reduce the reliability of our analysis. Therefore, we removed any columns that exceed this threshold. This ensures that our dataset remains focused on relevant and reliable information, enhancing the quality of our insights.\n\njob_postings.dropna(thresh = len(job_postings) * 0.5, axis = 1, inplace = True)\n\n\n\n\nCategorical fields, such as TITLE_RAW, were filled with “Unknown” for missing values. This approach allows us to retain the integrity of the dataset without introducing bias from arbitrary values. By labeling missing categorical data as “Unknown”, we can still analyze trends without losing valuable information.\n\njob_postings['TITLE_RAW'] = job_postings['TITLE_RAW'].fillna(\"Unknown\")\njob_postings['TITLE_CLEAN'] = job_postings['TITLE_CLEAN'].fillna(\"Unknown\")\njob_postings['COMPANY_RAW'] = job_postings['COMPANY_RAW'].fillna(\"Unknown\")\njob_postings['MSA_NAME'] = job_postings['MSA_NAME'].fillna(\"Unknown\")\njob_postings['MSA_NAME_OUTGOING'] = job_postings['MSA_NAME_OUTGOING'].fillna(\"Unknown\")\njob_postings['MSA_NAME_INCOMING'] = job_postings['MSA_NAME_INCOMING'].fillna(\"Unknown\")\n\n\n\n\nFor the EXPIRED variable, we chose to fill the missing values with the maximum date from this column. We assumed that the missing value here is because the post has not expired yet. By using the maximum date, we can effectively handle missing values without introducing bias or skewing the results.\n\njob_postings['POSTED'] = pd.to_datetime(job_postings['POSTED'])\njob_postings['EXPIRED'] = pd.to_datetime(job_postings['EXPIRED'])\n\n\nmax_expired_date = job_postings['EXPIRED'].max()\njob_postings['EXPIRED'] = job_postings['EXPIRED'].fillna(max_expired_date)\n\n\n\n\nFor the MIN_YEARS_EXPERIENCE variable, we chose to fill the missing values with the median MIN_YEARS_EXPERIENCE for a specific title or industry, similar to how we did with the SALARY variable. This can minimize the impact of outliers and provides a more accurate representation of the typical years of experience required for each job title.\n\ntitle_median_exp = job_postings.groupby('TITLE_NAME')['MIN_YEARS_EXPERIENCE'].median()\nindustry_median_exp = job_postings.groupby('NAICS_2022_6_NAME')['MIN_YEARS_EXPERIENCE'].median()\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: title_median_exp[row['TITLE_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['TITLE_NAME'] in title_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: industry_median_exp[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['NAICS_2022_6_NAME'] in industry_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings['MIN_YEARS_EXPERIENCE'].fillna(job_postings[\"MIN_YEARS_EXPERIENCE\"].median())\n\nDURATION variable is also a numerical field, but it has a different approach. We will fill the missing values with the difference between the POSTED and EXPIRED, which calculates the actual time span based on the available dates.\n\ndef impute_duration(cols):\n    posted = cols.iloc[0]\n    expired = cols.iloc[1]\n    duration = cols.iloc[2]\n\n    if pd.isnull(duration):\n        return expired - posted\n    else: \n        return duration\n\n\njob_postings['DURATION'] = job_postings[['POSTED', 'EXPIRED', 'DURATION']].apply(impute_duration, axis = 1)\n\n\n\n\n\n\n\n\n\njob_postings = job_postings.drop_duplicates(subset=[\"TITLE_NAME\", \"COMPANY_NAME\", \"LOCATION\", \"POSTED\"], keep = \"first\")\n\n\njob_postings.to_csv('job_postings_cleaned.csv', index=False)"
  },
  {
    "objectID": "data_cleaning.html#dropping-unnecessary-columns",
    "href": "data_cleaning.html#dropping-unnecessary-columns",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Specifically, we removed:\n- Older NAICS and SOC codes (e.g., NAICS2, SOC_2).\nThe North American Industry Classification System (NAICS) and Standard Occupational Classification (SOC) systems undergo periodic updates. Retaining only NAICS_2022_6 and SOC_2021_4 ensures we use the most recent classification standards. Moreover, older codes are redundant and may lead to inconsistencies in trend analysis.\n- Tracking data and URLs (e.g., ID, DUPLICATES).\nThese columns related to data collection timestamps, unique identifiers, or internal system references, which do not contribute to meaningful insights about the job market. Similarly, URLs are not necessary for our analysis as they do not provide any additional value or context but add unnecessary complexity to the dataset.\n\n\n\nThere are some reasons for this. Firstly, keeping only the latest industry and occupation classifications ensures our analysis reflects the most recent classification standards and avoid confusion and inconsistencies in classification. Additionally, reducing unnecessary columns speeds up data processing and enhances readability. This is particularly important when working with large datasets, as it minimizes the risk of errors and improves the efficiency of our analysis. Finally, it helps to focus on the most relevant information, allowing for clearer insights and conclusions regarding job market trends.\n\n\n\nBy removing outdated and irrelevant columns, we achieve:\n- More accurate job market trends, focusing on meaningful variables.\n- Easier interpretation without clutter from redundant or technical fields.\n- Faster analysis and visualization, improving overall efficiency.\n\njob_postings = pd.read_csv('lightcast_job_postings.csv')\n\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\", \"ACTIVE_URLS\", \"TITLE\", \"COMPANY\",\n    \"MSA\", \"STATE\", \"COUNTY\", \"CITY\", \"COUNTY_OUTGOING\", \"COUNTY_INCOMING\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    \"ONET\", \"ONET_2019\", \"CIP2\", \"CIP4\", \"CIP6\", \"MODELED_DURATION\", \"MODELED_EXPIRED\",\n    \"CERTIFICATIONS\", \"COMMON_SKILLS\", \"SPECIALIZED_SKILLS\", \"SKILLS\", \"SOFTWARE_SKILLS\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_OCCUPATION\", \"LOT_CAREER_AREA\",\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\",\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\", \"NAICS_2022_4\", \"NAICS_2022_4_NAME\",\n    \"NAICS_2022_5\", \"NAICS_2022_5_NAME\", \"NAICS_2022_6\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_5\", \"SOC_5_NAME\", \"SOC_4\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\", \"SOC_2021_4\"\n]\n\njob_postings.drop(columns = columns_to_drop, inplace = True)"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "plt.figure(figsize=(8, 6))\nmsno.heatmap(job_postings)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nThe SALARY column has a significant number of missing values. To handle this, we replaced the missing values with the median salary for that specific title or industry. This approach is effective because it minimizes the impact of outliers and provides a more accurate representation of the typical salary for each job title.\n\ntitle_median_salary = job_postings.groupby('TITLE_NAME')['SALARY'].median()\nindustry_median_salary = job_postings.groupby('NAICS_2022_6_NAME')['SALARY'].median()\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: title_median_salary[row['TITLE_NAME']]\n    if pd.isna(row['SALARY']) and row['TITLE_NAME'] in title_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: industry_median_salary[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['SALARY']) and row['NAICS_2022_6_NAME'] in industry_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'] = job_postings['SALARY'].fillna(job_postings[\"SALARY\"].median())\n\n\n\n\nDealing with columns that have more than 50% missing values is crucial for maintaining the integrity of our dataset. Columns with excessive missing data can introduce bias and reduce the reliability of our analysis. Therefore, we removed any columns that exceed this threshold. This ensures that our dataset remains focused on relevant and reliable information, enhancing the quality of our insights.\n\njob_postings.dropna(thresh = len(job_postings) * 0.5, axis = 1, inplace = True)\n\n\n\n\nCategorical fields, such as TITLE_RAW, were filled with “Unknown” for missing values. This approach allows us to retain the integrity of the dataset without introducing bias from arbitrary values. By labeling missing categorical data as “Unknown”, we can still analyze trends without losing valuable information.\n\njob_postings['TITLE_RAW'] = job_postings['TITLE_RAW'].fillna(\"Unknown\")\njob_postings['TITLE_CLEAN'] = job_postings['TITLE_CLEAN'].fillna(\"Unknown\")\njob_postings['COMPANY_RAW'] = job_postings['COMPANY_RAW'].fillna(\"Unknown\")\njob_postings['MSA_NAME'] = job_postings['MSA_NAME'].fillna(\"Unknown\")\njob_postings['MSA_NAME_OUTGOING'] = job_postings['MSA_NAME_OUTGOING'].fillna(\"Unknown\")\njob_postings['MSA_NAME_INCOMING'] = job_postings['MSA_NAME_INCOMING'].fillna(\"Unknown\")\n\n\n\n\nFor the EXPIRED variable, we chose to fill the missing values with the maximum date from this column. We assumed that the missing value here is because the post has not expired yet. By using the maximum date, we can effectively handle missing values without introducing bias or skewing the results.\n\njob_postings['POSTED'] = pd.to_datetime(job_postings['POSTED'])\njob_postings['EXPIRED'] = pd.to_datetime(job_postings['EXPIRED'])\n\n\nmax_expired_date = job_postings['EXPIRED'].max()\njob_postings['EXPIRED'] = job_postings['EXPIRED'].fillna(max_expired_date)\n\n\n\n\nFor the MIN_YEARS_EXPERIENCE variable, we chose to fill the missing values with the median MIN_YEARS_EXPERIENCE for a specific title or industry, similar to how we did with the SALARY variable. This can minimize the impact of outliers and provides a more accurate representation of the typical years of experience required for each job title.\n\ntitle_median_exp = job_postings.groupby('TITLE_NAME')['MIN_YEARS_EXPERIENCE'].median()\nindustry_median_exp = job_postings.groupby('NAICS_2022_6_NAME')['MIN_YEARS_EXPERIENCE'].median()\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: title_median_exp[row['TITLE_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['TITLE_NAME'] in title_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: industry_median_exp[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['NAICS_2022_6_NAME'] in industry_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings['MIN_YEARS_EXPERIENCE'].fillna(job_postings[\"MIN_YEARS_EXPERIENCE\"].median())\n\nDURATION variable is also a numerical field, but it has a different approach. We will fill the missing values with the difference between the POSTED and EXPIRED, which calculates the actual time span based on the available dates.\n\ndef impute_duration(cols):\n    posted = cols.iloc[0]\n    expired = cols.iloc[1]\n    duration = cols.iloc[2]\n\n    if pd.isnull(duration):\n        return expired - posted\n    else: \n        return duration\n\n\njob_postings['DURATION'] = job_postings[['POSTED', 'EXPIRED', 'DURATION']].apply(impute_duration, axis = 1)"
  },
  {
    "objectID": "data_cleaning.html#removing-duplicate-job-postings",
    "href": "data_cleaning.html#removing-duplicate-job-postings",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "job_postings = job_postings.drop_duplicates(subset=[\"TITLE_NAME\", \"COMPANY_NAME\", \"LOCATION\", \"POSTED\"], keep = \"first\")\n\n\njob_postings.to_csv('job_postings_cleaned.csv', index=False)"
  },
  {
    "objectID": "data_cleaning.html#top-20-companies-by-job-postings",
    "href": "data_cleaning.html#top-20-companies-by-job-postings",
    "title": "Data Cleaning & Exploration",
    "section": "2.1 Top 20 companies by job postings",
    "text": "2.1 Top 20 companies by job postings\n\nfiltered_companies = job_postings[job_postings[\"COMPANY_NAME\"] != \"Unclassified\"]\n\ntop_companies = filtered_companies[\"COMPANY_NAME\"].value_counts().head(20)\n\nfig = px.bar(\n    x=top_companies.values,\n    y=top_companies.index,\n    orientation='h',\n    title=\"Top 20 Companies by Job Postings (Excluding Unclassified)\",\n    labels={'x': 'Number of Job Postings', 'y': 'Company Name'},\n    text=top_companies.values\n)\n\nfig.update_layout(\n    xaxis_title=\"Number of Job Postings\",\n    yaxis_title=\"Company\",\n    yaxis={'categoryorder': 'total ascending'}, \n    height=600, \n    width=900\n)\n\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nThe visualization of the top 20 companies by job postings (excluding “Unclassified”) highlights key trends in the job market, particularly in the increasing demand for AI-related roles. Many of the companies with the most postings—Deloitte, Accenture, PricewaterhouseCoopers (PwC), Oracle, Infosys, Meta, and CDW—are major players in technology, consulting, and digital transformation, sectors that have been heavily investing in AI, machine learning, and data-driven innovation.\nThe dominance of these companies in job postings suggests that careers in AI and technology-related fields are in high demand. Consulting giants like Deloitte, Accenture, PwC, and KPMG are actively expanding their AI divisions, helping businesses integrate AI into their operations. For instance, Deloitte has launched several AI tools, including chatbots like “DARTbot” for audit professionals and “NavigAite” for document review, to enhance efficiency and client services (Stokes, 2025). Additionally, companies like Meta are pioneers in AI research, focusing on areas such as generative AI, automation, and data science. Even in non-tech sectors, financial and healthcare firms such as Citigroup, Cardinal Health, and Blue Cross Blue Shield are leveraging AI for fraud detection, risk assessment, and personalized healthcare.\nThese trends indicate that pursuing a career in AI-related fields, such as data science, machine learning engineering, and AI research, could provide greater job opportunities and higher earning potential. The strong presence of technology and consulting firms in job postings reflects how AI is becoming a fundamental part of business strategies across industries. While traditional, non-AI careers will continue to exist, the rapid push toward automation and intelligent systems suggests that AI-related skills will be increasingly valuable in both technical and non-technical roles. As industries continue adopting AI, professionals who develop expertise in this area may have a competitive advantage in the evolving job market."
  },
  {
    "objectID": "data_cleaning.html#salary-distribution-by-industry",
    "href": "data_cleaning.html#salary-distribution-by-industry",
    "title": "Data Cleaning & Exploration",
    "section": "2.2 Salary Distribution by Industry",
    "text": "2.2 Salary Distribution by Industry\n\nfig = px.box(job_postings, x=\"NAICS_2022_6_NAME\", y=\"SALARY\", title=\"Salary Distribution by Industry\")\nfig.update_layout(width=1200, height=1000)\nfig.show()\n\n                            \n                                            \n\n\nThe box plot provides a clearer view of salary distributions across industries, highlighting variations in median salaries and outliers. Most industries exhibit salary concentrations below $200K, with some sectors showing significantly higher outliers above $300K-$500K, suggesting high-paying roles in specialized fields.\nAI-related jobs, typically found in industries such as technology, finance, and advanced manufacturing, often contribute to these high-salary outliers. Roles in machine learning, data science, and artificial intelligence engineering command premium salaries due to their specialized skill requirements, talent scarcity, and high demand across multiple industries. The broader salary spread in AI-intensive fields may also reflect differences in job seniority, from entry-level analysts to highly compensated AI researchers and executives.\nAdditionally, AI-driven industries tend to offer competitive compensation to attract top talent, given the rapid pace of technological advancement and the strategic importance of AI in business growth. The dense clustering of lower salaries in non-AI industries indicates a more constrained range, potentially due to standardized pay structures or lower technical barriers to entry."
  },
  {
    "objectID": "data_cleaning.html#top-5-occupations-by-average-salary",
    "href": "data_cleaning.html#top-5-occupations-by-average-salary",
    "title": "Data Cleaning & Exploration",
    "section": "2.3 Top 5 Occupations by Average Salary",
    "text": "2.3 Top 5 Occupations by Average Salary\n\navg_salary_per_occupation = job_postings.groupby(\"LOT_V6_OCCUPATION_NAME\")[\"SALARY\"].mean().reset_index()\n\ntop_occupations = avg_salary_per_occupation.sort_values(by=\"SALARY\", ascending=False).head(5)\n\nfig = px.bar(\n        top_occupations,\n        x=\"SALARY\",\n        y=\"LOT_V6_OCCUPATION_NAME\",\n        orientation='h',\n        title=\"Top 5 Occupations by Average Salary\",\n        labels={\"SALARY\": \"Average Salary ($)\", \"LOT_V6_OCCUPATION_NAME\": \"Occupation\"},\n        text=top_occupations[\"SALARY\"]\n    )\n\nfig.update_layout(\n        xaxis_title=\"Average Salary ($)\",\n        yaxis_title=\"Occupation\",\n        yaxis={\"categoryorder\": \"total ascending\"}, \n        height=700,\n        width=900\n    )\n\nfig.show()\n\n                            \n                                            \n\n\nThe salary distribution in the graph clearly shows that the highest-paying occupations are directly tied to artificial intelligence, data analytics, and business intelligence. The top-paying role, “Computer Systems Engineer / Architect,” averages over $156,000, followed by “Business Intelligence Analyst” at $125,000 and other AI-driven roles like “Data Mining Analyst” and “Market Research Analyst,” all exceeding $100,000. These occupations rely heavily on AI, machine learning, and data-driven decision-making, making it clear that mastering AI-related skills is directly linked to higher salaries. The strong earnings for these roles indicate that industries are willing to pay a premium for professionals who can build, interpret, and optimize AI-driven systems.\nIn contrast, traditional non-AI careers, which are not as data or automation-focused, tend to fall outside these top salary brackets. The job market is shifting towards AI dependency, where knowing how to work with artificial intelligence, big data, and automation tools is no longer just an advantage but a necessity for higher-paying opportunities. As industries integrate AI at an increasing pace, professionals who fail to develop AI-related expertise risk stagnating in lower-paying roles, while those who embrace AI technologies position themselves for significantly better financial rewards."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "The global job market is undergoing a significant transformation due to various factors, including the rapid adoption of artificial intelligence (AI), evolving work models such as remote work, and shifts in industry-specific wage structures. For job seekers, understanding these dynamics is crucial for making informed decisions about career paths, salary expectations, and work environment preferences. This research aims to analyze salary trends for AI and non-AI careers, as well as the impact of remote work, regional differences, and specific industry on salaries.\n\n\n\nAs the economy adapts to AI technologies, many industries are experiencing shifts in job structures and salary scales. This topic is important because job seekers need to be aware of the changing landscape to make career decisions that align with future trends and personal goals. These years, the rise of AI and automation is creating new job roles while displacing others. Moreover, remote work, which played an important role during the pandemic, has become a permanent feature for many industries. However, questions remain about how compensation for remote roles compares to in-office positions and how it varies across regions and industries. Based on typical trends on the job market, this analysis will explore several key findings:\n\nHow do salaries differ across AI vs. non-AI careers?\nWhat regions offer the highest-paying jobs in AI-related and traditional careers?\nAre remote jobs better paying than in-office roles?\nWhat industries saw the biggest wage growth in 2024?"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "The global job market is undergoing a significant transformation due to various factors, including the rapid adoption of artificial intelligence (AI), evolving work models such as remote work, and shifts in industry-specific wage structures. For job seekers, understanding these dynamics is crucial for making informed decisions about career paths, salary expectations, and work environment preferences. This research aims to analyze salary trends for AI and non-AI careers, as well as the impact of remote work, regional differences, and specific industry on salaries."
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "As the economy adapts to AI technologies, many industries are experiencing shifts in job structures and salary scales. This topic is important because job seekers need to be aware of the changing landscape to make career decisions that align with future trends and personal goals. These years, the rise of AI and automation is creating new job roles while displacing others. Moreover, remote work, which played an important role during the pandemic, has become a permanent feature for many industries. However, questions remain about how compensation for remote roles compares to in-office positions and how it varies across regions and industries. Based on typical trends on the job market, this analysis will explore several key findings:\n\nHow do salaries differ across AI vs. non-AI careers?\nWhat regions offer the highest-paying jobs in AI-related and traditional careers?\nAre remote jobs better paying than in-office roles?\nWhat industries saw the biggest wage growth in 2024?"
  },
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom pyspark.sql import SparkSession\nimport plotly.io as pio\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\nfrom tabulate import tabulate\nfrom IPython.display import HTML\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, pow, sqrt, abs, mean, avg, sum as spark_sum, round as spark_round, row_number\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nimport plotly.graph_objects as go\n\n\nplotly_layout = dict(\n    font=dict(family=\"Arial\", size=14),\n    title_font=dict(size=20, family=\"Arial\", color=\"black\"),\n    paper_bgcolor=\"white\",\n    plot_bgcolor=\"white\",\n    margin=dict(t=60, l=60, r=30, b=60),\n    legend=dict(bordercolor=\"lightgray\", borderwidth=1),\n    xaxis=dict(\n        title_font=dict(size=16, color=\"black\"),  \n        tickfont=dict(size=12, color=\"black\"),    \n        showgrid=True, \n        gridcolor=\"lightgray\",\n        showline=True,\n        linecolor=\"black\",\n        linewidth=1,\n        mirror=True,\n        zeroline=True,\n        zerolinecolor=\"gray\",\n        zerolinewidth=1\n    ),\n    yaxis=dict(\n        title_font=dict(size=16, color=\"black\"),  \n        tickfont=dict(size=12, color=\"black\"),    \n        showgrid=True, \n        gridcolor=\"lightgray\",\n        showline=True,\n        linecolor=\"black\",\n        linewidth=1,\n        mirror=True,\n        zeroline=True,\n        zerolinecolor=\"gray\",\n        zerolinewidth=1\n    ),\n)\n\n\n1 Unsupervised Learning: KMeans Clustering\nIn this section, we used KMeans clustering to group job postings based on Minimum Years of Experience and Salary. Our goal was to find natural patterns in how job roles are distributed across different experience and salary ranges. After clustering, we used NAICS6_NAME industry classifications to interpret the types of industries represented in each cluster.\nBy doing so, we can better understand how different industries vary in their experience requirements and compensation levels, providing insights into salary structures across the job market.\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\n# df.printSchema() \n# df.show(5)\n\n# Register the DataFrame as a temporary SQL table\ndf.createOrReplaceTempView(\"job_postings\")\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/30 00:49:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[Stage 0:===========================================================(1 + 0) / 1]                                                                                [Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                25/04/30 00:50:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n\ndf = df.dropna(subset=['MIN_YEARS_EXPERIENCE', 'SALARY', 'NAICS6_NAME'])\ndf = df.filter(df.NAICS6_NAME != 'Unclassified Industry')\n\ndf_casted = df.select(\n    col(\"MIN_YEARS_EXPERIENCE\").cast(DoubleType()),\n    col(\"SALARY\").cast(DoubleType()),\n    col(\"NAICS6_NAME\")\n)\n\nassembler = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"SALARY\"], \n    outputCol='features_unscaled'\n)\ndf_features = assembler.transform(df_casted)\n\n\nscaler = StandardScaler(\n    inputCol='features_unscaled', \n    outputCol='features', \n    withMean=True, \n    withStd=True\n)\nscaler_model = scaler.fit(df_features)\ndf_scaled = scaler_model.transform(df_features)\n\n[Stage 2:&gt;                                                          (0 + 1) / 1][Stage 2:===========================================================(1 + 0) / 1]                                                                                \n\n\n\nscores = []\nks = list(range(2, 9))\n\nfor k in ks:\n    kmeans = KMeans().setK(k).setSeed(42).setFeaturesCol(\"features\")\n    model = kmeans.fit(df_scaled)\n    transformed = model.transform(df_scaled)\n\n    evaluator = ClusteringEvaluator(\n        featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n    score = evaluator.evaluate(transformed)\n    scores.append(score)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=ks, y=scores, mode='lines+markers', name='Silhouette Score'))\nfig.update_layout(title=\"Silhouette Score vs. k\", \n                  xaxis_title=\"k\",\n                  yaxis_title=\"Score\",\n                  template=\"simple_white\",\n                  width=900)\nfig.update_layout(**plotly_layout) \nfig.show()\n\n[Stage 5:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 8:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 40:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 43:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 45:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 48:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 51:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 99:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 102:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 104:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 107:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 110:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 158:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 161:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 163:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 166:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 169:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 217:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 220:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 222:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 225:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 228:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 270:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 273:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 275:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 278:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 281:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 329:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 332:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 334:&gt;                                                        (0 + 1) / 1][Stage 334:=========================================================(1 + 0) / 1]                                                                                [Stage 337:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 340:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 388:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 391:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 393:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nkmeans_final = KMeans(featuresCol='features', k=4, seed=688)\nmodel_final = kmeans_final.fit(df_scaled)\n\npredictions = model_final.transform(df_scaled)\n\n[Stage 396:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 399:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 445:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n\nindustry_counts = predictions.groupBy('prediction', 'NAICS6_NAME').count()\n\nwindow_spec = Window.partitionBy('prediction')\nindustry_counts = industry_counts.withColumn('total', spark_sum('count').over(window_spec))\n\nindustry_counts = industry_counts.withColumn('percentage', spark_round(col('count') / col('total') * 100, 2))\n\nwindow_top5 = Window.partitionBy('prediction').orderBy(col('percentage').desc())\nindustry_top5 = industry_counts.withColumn('row_num', row_number().over(window_top5)).filter(col('row_num') &lt;= 5)\n\nindustry_top5.orderBy('prediction', 'row_num').show(100, truncate=False)\n\n[Stage 448:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n+----------+--------------------------------------------------------------------+-----+-----+----------+-------+\n|prediction|NAICS6_NAME                                                         |count|total|percentage|row_num|\n+----------+--------------------------------------------------------------------+-----+-----+----------+-------+\n|0         |Computer Systems Design Services                                    |1243 |3919 |31.72     |1      |\n|0         |Administrative Management and General Management Consulting Services|219  |3919 |5.59      |2      |\n|0         |Custom Computer Programming Services                                |177  |3919 |4.52      |3      |\n|0         |Drugs and Druggists' Sundries Merchant Wholesalers                  |163  |3919 |4.16      |4      |\n|0         |Direct Health and Medical Insurance Carriers                        |159  |3919 |4.06      |5      |\n|1         |Administrative Management and General Management Consulting Services|683  |2605 |26.22     |1      |\n|1         |Web Search Portals and All Other Information Services               |188  |2605 |7.22      |2      |\n|1         |Commercial Banking                                                  |165  |2605 |6.33      |3      |\n|1         |Employment Placement Agencies                                       |143  |2605 |5.49      |4      |\n|1         |Offices of Certified Public Accountants                             |139  |2605 |5.34      |5      |\n|2         |Administrative Management and General Management Consulting Services|573  |7828 |7.32      |1      |\n|2         |Employment Placement Agencies                                       |486  |7828 |6.21      |2      |\n|2         |Direct Health and Medical Insurance Carriers                        |404  |7828 |5.16      |3      |\n|2         |Colleges, Universities, and Professional Schools                    |399  |7828 |5.1       |4      |\n|2         |Custom Computer Programming Services                                |367  |7828 |4.69      |5      |\n|3         |Administrative Management and General Management Consulting Services|1283 |6912 |18.56     |1      |\n|3         |Employment Placement Agencies                                       |435  |6912 |6.29      |2      |\n|3         |Commercial Banking                                                  |378  |6912 |5.47      |3      |\n|3         |Computer Systems Design Services                                    |335  |6912 |4.85      |4      |\n|3         |Direct Health and Medical Insurance Carriers                        |333  |6912 |4.82      |5      |\n+----------+--------------------------------------------------------------------+-----+-----+----------+-------+\n\n\n\n\npandas_df = predictions.select('MIN_YEARS_EXPERIENCE', 'SALARY', 'prediction').toPandas()\n\npandas_df['Cluster_Name'] = pandas_df['prediction'].map({0: 'Cluster 1', \n                                                         1: 'Cluster 2', \n                                                         2: 'Cluster 3',\n                                                         3: 'Cluster 4'})\n\nplt.figure(figsize=(8, 5))\nfig = px.scatter(\n    pandas_df, \n    x=\"MIN_YEARS_EXPERIENCE\", \n    y=\"SALARY\", \n    color=\"Cluster_Name\", \n    title=\"K-Means Clustering on Job Postings Data\", \n    labels={\n        \"MIN_YEARS_EXPERIENCE\": \"Minimum Years of Experience\", \n        \"SALARY\": \"Salary\",\n        \"Cluster_Name\": \"Cluster\"\n    },\n    category_orders={\"Cluster_Name\": [\"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Cluster 4\"]}\n)\n\nfig.update_layout(**plotly_layout,\n                  width=800,\n                  height=500) \nfig.show()\n\n[Stage 454:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n                            \n                                            \n\n\n&lt;Figure size 768x480 with 0 Axes&gt;\n\n\nBased on the silhouette score of the K value and practical anlysis requirements, we identified four distinct clusters that capture major compensation patterns in the labor market.\nHere are key findings based on salary and experience trends:\n\nCluster 1:\n\nExperience/Salary Pattern: Requires higher minimum years of experience. Offers only moderate salary levels despite higher experience.\n\nTop Industries: Computer Systems Design Services, Administrative Management and General Management Consulting Services, and Custom Computer Programming Services.\n\nInsight: Jobs demanding significant prior experience but offering relatively moderate compensation. Indicates competitive markets in tech and consulting sectors.\n\n\nCluster 2:\n\nExperience/Salary Pattern: Consistently the highest salaries across a wide range of experience levels.\n\nTop Industries: Administrative Management and General Management Consulting Services, Web Search Portals and Other Information Services, and Commercial Banking.\n\nInsight: Reflects premium-paying roles in consulting, web services, and finance. This suggests opportunities for substantial earnings even with moderate experience.\n\n\nCluster 3:\n\nExperience/Salary Pattern: Requires lower years of experience. Salary levels are generally the lowest.\n\nTop Industries: Administrative Management and General Management Consulting Services, Employment Placement Agencies, and Direct Health and Medical Insurance Carriers.\n\nInsight: Entry-level or early-career roles in sectors with limited immediate salary growth.\n\n\nCluster 4:\n\nExperience/Salary Pattern: Moderate years of experience required. Salary levels are moderately high.\n\nTop Industries: Administrative Management and General Management Consulting Services, Employment Placement Agencies, and Commercial Banking.\n\nInsight: Steady career tracks offering good compensation for mid-experience professionals.\n\n\nImplications for Salary and Compensation Trends:\n- Salary growth is not always linear with experience; certain clusters show salary plateaus despite increasing experience.\n- Industry effects are significant: sectors like Professional Services and Finance consistently appear across clusters, but compensation levels vary depending on experience requirements.\n- High-paying opportunities exist both at low and high experience levels, depending on industry and role specialization.\nImplications for Job Seekers:\n- High Salary Aspirations: Target roles in Cluster 2 industries like consulting, finance, and web services where premium salaries are achievable even with moderate experience.\n\nCareer Launch: Cluster 3 industries may provide easier entry points for new graduates but with lower starting salaries. In contrast, positions in Cluster 4 offer a good balance between experience investment and salary rewards.\nBeware of High-Experience/Moderate-Pay Sectors: Cluster 1 jobs may require significant experience without corresponding salary premiums, requiring careful career planning.\n\n\n\n2 Supervised Learning: Random Forest Regression\nTo deepen our analysis on Salary and Compensation Trends, we constructed a Random Forest Regression model using salary as the target variable. The goal of this model is to predict salary outcomes based on key job posting attributes, and to identify the relative importance of different factors influencing compensation in the labor market.\nThe predictor variables selected for the model include: DURATION, MIN_YEARS_EXPERIENCE, LOT_V6_OCCUPATION_NAME, STATE_NAME, EMPLOYMENT_TYPE_NAME\n\ndf_rf = df.dropna(subset=['DURATION', 'MIN_YEARS_EXPERIENCE',  \n                          'LOT_V6_OCCUPATION_NAME', 'STATE_NAME', 'EMPLOYMENT_TYPE_NAME',\n                          'SALARY'])\n\ncategorical_cols = ['LOT_V6_OCCUPATION_NAME', 'STATE_NAME', 'EMPLOYMENT_TYPE_NAME'] \ncontinuous_cols = ['DURATION', 'MIN_YEARS_EXPERIENCE'] \n\n# Index and One-Hot Encode\nindexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid='skip') for col in categorical_cols]\nencoders = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_vec\") for col in categorical_cols]\n\n# Assemble base features \nassembler = VectorAssembler(\n    inputCols=continuous_cols \n    + [f\"{col}_vec\" for col in categorical_cols], \n    outputCol=\"features\"\n)\n\n# Build pipeline and transform\npipeline = Pipeline(stages=indexers + encoders + [assembler]) \ndata = pipeline.fit(df_rf).transform(df_rf)\n\n# Show final structure\ndata.select(\"SALARY\", \"features\").show(5, truncate=False)\n\n[Stage 455:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 458:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 461:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n+------+------------------------------------------+\n|SALARY|features                                  |\n+------+------------------------------------------+\n|192800|(59,[0,1,2,15,57],[55.0,6.0,1.0,1.0,1.0]) |\n|125900|(59,[0,1,2,16,57],[18.0,12.0,1.0,1.0,1.0])|\n|118560|(59,[0,1,3,28,57],[20.0,5.0,1.0,1.0,1.0]) |\n|192800|(59,[0,1,2,13,57],[55.0,6.0,1.0,1.0,1.0]) |\n|116500|(59,[0,1,2,16,57],[16.0,12.0,1.0,1.0,1.0])|\n+------+------------------------------------------+\nonly showing top 5 rows\n\n\n\n\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=688)\n\nrf = RandomForestRegressor(featuresCol=\"features\",\n                           labelCol=\"SALARY\", \n                           numTrees=150,\n                           maxDepth=9, \n                           seed=688 \n                           )\n\n# Train model\nrf_model = rf.fit(train_data.select(\"SALARY\", \"features\"))\n\n# Generate predictions\nrf_preds = rf_model.transform(train_data.select(\"SALARY\", \"features\"))\n\n[Stage 465:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 466:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 467:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 469:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 473:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 475:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 477:&gt;                                                        (0 + 1) / 1]                                                                                25/04/30 00:57:35 WARN DAGScheduler: Broadcasting large task binary with size 1655.1 KiB\n[Stage 479:&gt;                                                        (0 + 1) / 1][Stage 480:&gt;                                                        (0 + 1) / 1]                                                                                25/04/30 00:57:37 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n[Stage 481:&gt;                                                        (0 + 1) / 1][Stage 482:&gt;                                                        (0 + 1) / 1]                                                                                25/04/30 00:57:41 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n[Stage 483:&gt;                                                        (0 + 1) / 1][Stage 484:&gt;                                                        (0 + 1) / 1]                                                                                25/04/30 00:57:45 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n[Stage 485:&gt;                                                        (0 + 1) / 1][Stage 486:&gt;                                                        (0 + 1) / 1]                                                                                WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/spark-3.5.4-bin-hadoop3/jars/spark-core_2.12-3.5.4.jar) to field java.nio.charset.Charset.name\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n\n\n\n# Extract feature importances\ndef get_actual_feature_names(df_rf, assembler, encoded_cols):\n    full_feature_names = []\n\n    for col_name in assembler.getInputCols():\n        if col_name in encoded_cols:\n            try:\n                attr_meta = df_rf.schema[col_name].metadata['ml_attr']['attrs']\n                for attr_group in attr_meta.values():\n                    for attr in attr_group:\n                        full_feature_names.append(attr['name'])\n            except:\n                full_feature_names.append(col_name)\n        else:\n            full_feature_names.append(col_name)\n\n    return full_feature_names\n\nencoded_cols = [f\"{col}_vec\" for col in categorical_cols] \nfeature_names = get_actual_feature_names(data, assembler, encoded_cols)\nimportances = rf_model.featureImportances.toArray() \n\n\ndef clean_feature_names(feature_list):\n    clean_names = []\n    for name in feature_list:\n        if isinstance(name, list):\n            clean_names.append(\", \".join(str(n) for n in name))\n        elif isinstance(name, str) and name.startswith(\"[\"):\n            clean_names.append(name.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', '').strip())\n        else:\n            clean_names.append(str(name))\n    return clean_names\n\n# Build dataframe\nimportance_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": importances\n}).sort_values(by=\"Importance\", ascending=False)\n\nimportance_df[\"Feature\"] = clean_feature_names(importance_df[\"Feature\"])\ntop_importance_df = importance_df.head(15)\n\n# Plot\nplt.figure(figsize=(9, 6))\nsns.barplot(\n    x=\"Importance\",\n    y=\"Feature\",\n    data=top_importance_df,\n    hue=\"Feature\",\n    palette=\"viridis\"\n)\n\nimport textwrap\nlabels = plt.gca().get_yticklabels()\nnew_labels = [textwrap.fill(label.get_text(), width=30) for label in labels]\nplt.yticks(range(len(new_labels)), new_labels, fontsize=9)\nplt.xticks(fontsize=10)\n\nplt.title(\"Top 15 Feature Importances from Random Forest Model\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Importance\", fontsize=12, fontweight='bold')\nplt.ylabel(\"Feature\", fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nevaluator_r2 = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"r2\")\n\nrf_preds = rf_model.transform(test_data)\nrf_residuals = rf_preds.select(\n    col(\"SALARY\"),\n    col(\"prediction\"),\n    (col(\"SALARY\") - col(\"prediction\")).alias(\"residual\")\n)\n\nrf_r2   = evaluator_r2.evaluate(rf_preds) \nrf_rmse = np.sqrt(rf_residuals.select(avg(pow(col(\"residual\"), 2))).first()[0]) \nrf_aic  = None\nrf_bic  = None\n\nrf_pdf = rf_residuals.select(\"SALARY\", col(\"prediction\").alias(\"RandomForest\")).toPandas()\nrf_df = pd.DataFrame({\"SALARY\": rf_pdf[\"SALARY\"], \"RandomForest\": rf_pdf[\"RandomForest\"]})\n\n[Stage 487:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 488:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 491:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n\nplt.figure(figsize=(7, 17))\nsns.set(style=\"whitegrid\")\n\nmodels = {\"RandomForest\": (rf_rmse, rf_r2, \"NA\", \"NA\")}\n\nmodel_dfs = {\"RandomForest\": rf_df}\n\nfor idx, (model_name, (rmse, r2, aic, bic)) in enumerate(models.items(), 1):\n    plt.subplot(3, 1, idx)\n    \n    model_data = model_dfs[model_name]\n    \n    sns.scatterplot(x=\"SALARY\", y=model_name, data=model_data, alpha=0.5, label=model_name)\n    \n    x_min = model_data[\"SALARY\"].min()\n    x_max = model_data[\"SALARY\"].max()\n    plt.plot([x_min, x_max], [x_min, x_max], 'r-', label=\"Ideal Fit\")\n    \n    if aic != \"NA\" and bic != \"NA\":\n        plt.title(f\"{model_name} Prediction\\nRMSE={rmse:.1f} | R²={r2:.3f} | AIC={aic:.1f} | BIC={bic:.1f}\", fontweight=\"bold\")\n    else:\n        plt.title(f\"{model_name} Prediction\\nRMSE={rmse:.1f} | R²={r2:.3f} | AIC=NA | BIC=NA\", fontweight=\"bold\")\n    \n    plt.xlabel(\"Actual Salary\", fontweight=\"bold\")\n    plt.ylabel(\"Predicted Salary\", fontweight=\"bold\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nModel Evaluation:\n- The R-Squared of 0.454 indicates a moderate level of predictive power, suggesting that the model captures a substantial portion of salary variability but leaves room for improvement.\n- The scatterplot shows that most predictions are reasonably aligned with the actual salaries but tend to underpredict higher salary values (especially above $200,000), which is common due to the small number of very high salaries (“long tail” effect).\nFeature Importance Analysis:\n- The top 15 feature importances from random forest model show how different factors contribute to salary predictions. In our model, years of experience are by far the strongest predictor of salary. Having some certain occupation titles can also greatly affect salary expectations. Interestingly, the job posting duration is also an important factor in salary prediction, which may be related to stable roles.\n- On the other hand, geographic location and job type have less impact on salary predictions, with states like Oregon and California showing specific salary patterns.\nImplications for Job Seekers:\n- Experience Pays Off: The model shows that minimum years of experience is the dominant factor influencing salary. For job seekers, gaining and accurately showcasing professional experience is crucial to achieving higher salary outcomes. Moreover, investing in data-related skills can be a smart career move.\n- Occupation Choice Matters: Specific technical roles (especially Computer Systems Engineers, Data Analysts, and Business Intelligence Analysts) are associated with higher salaries. Choosing high-demand, specialized roles can significantly improve salary prospects.\n- Location Strategy: While experience and occupation dominate, geography still plays a role, for example, states like Oregon, California, and New York affect salary expectations. Job seekers willing to relocate or negotiate for remote work with companies based in higher-paying states may gain salary advantages."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\njob_postings = pd.read_csv('job_postings_cleaned.csv')"
  },
  {
    "objectID": "eda.html#top-20-companies-by-job-postings",
    "href": "eda.html#top-20-companies-by-job-postings",
    "title": "Exploratory Data Analysis",
    "section": "1.1 Top 20 companies by job postings",
    "text": "1.1 Top 20 companies by job postings\n\nfiltered_companies = job_postings[job_postings[\"COMPANY_NAME\"] != \"Unclassified\"]\n\ntop_companies = filtered_companies[\"COMPANY_NAME\"].value_counts().head(20)\n\nfig = px.bar(\n    x=top_companies.values,\n    y=top_companies.index,\n    orientation='h',\n    title=\"Top 20 Companies by Job Postings (Excluding Unclassified)\",\n    labels={'x': 'Number of Job Postings', 'y': 'Company Name'},\n    text=top_companies.values\n)\n\nfig.update_layout(\n    xaxis_title=\"Number of Job Postings\",\n    yaxis_title=\"Company\",\n    yaxis={'categoryorder': 'total ascending'}, \n    height=600, \n    width=900\n)\n\nfig.update_layout(**plotly_layout) \nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nThe visualization of the top 20 companies by job postings (excluding “Unclassified”) highlights key trends in the job market, particularly in the increasing demand for AI-related roles. Many of the companies with the most postings—Deloitte, Accenture, PricewaterhouseCoopers (PwC), Oracle, Infosys, Meta, and CDW—are major players in technology, consulting, and digital transformation, sectors that have been heavily investing in AI, machine learning, and data-driven innovation.\nThe dominance of these companies in job postings suggests that careers in AI and technology-related fields are in high demand. Consulting giants like Deloitte, Accenture, PwC, and KPMG are actively expanding their AI divisions, helping businesses integrate AI into their operations. For instance, Deloitte has launched several AI tools, including chatbots like “DARTbot” for audit professionals and “NavigAite” for document review, to enhance efficiency and client services (Stokes, 2025). Additionally, companies like Meta are pioneers in AI research, focusing on areas such as generative AI, automation, and data science. Even in non-tech sectors, financial and healthcare firms such as Citigroup, Cardinal Health, and Blue Cross Blue Shield are leveraging AI for fraud detection, risk assessment, and personalized healthcare.\nThese trends indicate that pursuing a career in AI-related fields, such as data science, machine learning engineering, and AI research, could provide greater job opportunities and higher earning potential. The strong presence of technology and consulting firms in job postings reflects how AI is becoming a fundamental part of business strategies across industries. While traditional, non-AI careers will continue to exist, the rapid push toward automation and intelligent systems suggests that AI-related skills will be increasingly valuable in both technical and non-technical roles. As industries continue adopting AI, professionals who develop expertise in this area may have a competitive advantage in the evolving job market."
  },
  {
    "objectID": "eda.html#salary-distribution-by-industry",
    "href": "eda.html#salary-distribution-by-industry",
    "title": "Exploratory Data Analysis",
    "section": "1.2 Salary Distribution by Industry",
    "text": "1.2 Salary Distribution by Industry\n\nfig = px.box(job_postings, x=\"NAICS_2022_6_NAME\", y=\"SALARY\", title=\"Salary Distribution by Industry\")\nfig.update_layout(width=1200, height=1000)\nfig.update_layout(**plotly_layout)\nfig.show()\n\n                            \n                                            \n\n\nThe box plot provides a clearer view of salary distributions across industries, highlighting variations in median salaries and outliers. Most industries exhibit salary concentrations below $200K, with some sectors showing significantly higher outliers above $300K-$500K, suggesting high-paying roles in specialized fields.\nAI-related jobs, typically found in industries such as technology, finance, and advanced manufacturing, often contribute to these high-salary outliers. Roles in machine learning, data science, and artificial intelligence engineering command premium salaries due to their specialized skill requirements, talent scarcity, and high demand across multiple industries. The broader salary spread in AI-intensive fields may also reflect differences in job seniority, from entry-level analysts to highly compensated AI researchers and executives.\nAdditionally, AI-driven industries tend to offer competitive compensation to attract top talent, given the rapid pace of technological advancement and the strategic importance of AI in business growth. The dense clustering of lower salaries in non-AI industries indicates a more constrained range, potentially due to standardized pay structures or lower technical barriers to entry."
  },
  {
    "objectID": "eda.html#top-5-occupations-by-average-salary",
    "href": "eda.html#top-5-occupations-by-average-salary",
    "title": "Exploratory Data Analysis",
    "section": "1.3 Top 5 Occupations by Average Salary",
    "text": "1.3 Top 5 Occupations by Average Salary\n\navg_salary_per_occupation = job_postings.groupby(\"LOT_V6_OCCUPATION_NAME\")[\"SALARY\"].mean().reset_index()\n\ntop_occupations = avg_salary_per_occupation.sort_values(by=\"SALARY\", ascending=False).head(5)\n\nfig = px.bar(\n        top_occupations,\n        x=\"SALARY\",\n        y=\"LOT_V6_OCCUPATION_NAME\",\n        orientation='h',\n        title=\"Top 5 Occupations by Average Salary\",\n        labels={\"SALARY\": \"Average Salary ($)\", \"LOT_V6_OCCUPATION_NAME\": \"Occupation\"},\n        text=top_occupations[\"SALARY\"]\n    )\n\nfig.update_layout(\n        xaxis_title=\"Average Salary ($)\",\n        yaxis_title=\"Occupation\",\n        yaxis={\"categoryorder\": \"total ascending\"}, \n        height=700,\n        width=900\n    )\n\nfig.update_layout(**plotly_layout)\nfig.show()\n\n                            \n                                            \n\n\nThe salary distribution in the graph clearly shows that the highest-paying occupations are directly tied to artificial intelligence, data analytics, and business intelligence. The top-paying role, “Computer Systems Engineer / Architect,” averages over $156,000, followed by “Business Intelligence Analyst” at $125,000 and other AI-driven roles like “Data Mining Analyst” and “Market Research Analyst,” all exceeding $100,000. These occupations rely heavily on AI, machine learning, and data-driven decision-making, making it clear that mastering AI-related skills is directly linked to higher salaries. The strong earnings for these roles indicate that industries are willing to pay a premium for professionals who can build, interpret, and optimize AI-driven systems.\nIn contrast, traditional non-AI careers, which are not as data or automation-focused, tend to fall outside these top salary brackets. The job market is shifting towards AI dependency, where knowing how to work with artificial intelligence, big data, and automation tools is no longer just an advantage but a necessity for higher-paying opportunities. As industries integrate AI at an increasing pace, professionals who fail to develop AI-related expertise risk stagnating in lower-paying roles, while those who embrace AI technologies position themselves for significantly better financial rewards."
  },
  {
    "objectID": "eda.html#enhanced-visualizations",
    "href": "eda.html#enhanced-visualizations",
    "title": "Exploratory Data Analysis",
    "section": "1.4 Enhanced Visualizations",
    "text": "1.4 Enhanced Visualizations"
  },
  {
    "objectID": "eda.html#job-postings-trend-over-time-top-companies",
    "href": "eda.html#job-postings-trend-over-time-top-companies",
    "title": "Exploratory Data Analysis",
    "section": "1.5 Job Postings Trend Over Time (Top Companies)",
    "text": "1.5 Job Postings Trend Over Time (Top Companies)\n\njob_postings['POSTED'] = pd.to_datetime(job_postings['POSTED'])\ntop_companies = (\n    job_postings[job_postings[\"COMPANY_NAME\"] != \"Unclassified\"][\"COMPANY_NAME\"]\n    .value_counts()\n    .head(10)\n    .index\n)\n\nfiltered = job_postings[job_postings['COMPANY_NAME'].isin(top_companies)]\n\ntrend = (\n    filtered.groupby([filtered['POSTED'].dt.to_period('M'), 'COMPANY_NAME'])\n    .size()\n    .reset_index(name='Postings')\n)\ntrend['POSTED'] = trend['POSTED'].dt.to_timestamp()\n\nfig = px.line(trend, x='POSTED', y='Postings', color='COMPANY_NAME',\n              title='Monthly Job Postings for Top 10 Companies')\nfig.update_layout(**plotly_layout)\nfig.show()\n\n                            \n                                            \n\n\nThe line chart above reveals dynamic shifts in job posting activity among the top 10 hiring companies over recent months. Several key patterns emerge:\n\nInfosys shows a strong upward trend, indicating a possible expansion phase or increased demand for tech-related talent. This could reflect growing project loads or client demand in IT services and consulting.\nAccenture and Deloitte maintain relatively stable posting volumes, suggesting consistent hiring pipelines. This stability aligns with their roles as global consulting giants with ongoing needs for specialized talent in digital transformation, data analytics, and strategy.\nHumana and Insight Global exhibit moderate declines followed by slight recoveries, potentially pointing to seasonal or project-based hiring fluctuations in healthcare and staffing services.\nCompanies like KPMG, Oracle, and PricewaterhouseCoopers (PwC) show lower and flatter posting trends, possibly indicating a more conservative hiring approach or specific recruitment periods during the year.\nMerit America, a nonprofit focused on career advancement, remains on the lower end of the spectrum. However, its presence in the top 10 indicates consistent demand in educational or workforce development roles.\n\nOverall, the chart highlights Infosys as a standout, with its consistent rise suggesting aggressive recruitment. In contrast, other firms maintain steady or slightly fluctuating volumes, reflecting industry-specific hiring cycles. This trend-based view can be valuable for job seekers, workforce planners, or analysts studying labor market activity in the consulting, healthcare, tech, and staffing sectors."
  },
  {
    "objectID": "eda.html#salary-distribution-by-industry-filtered-outliers",
    "href": "eda.html#salary-distribution-by-industry-filtered-outliers",
    "title": "Exploratory Data Analysis",
    "section": "1.6 Salary Distribution by Industry (Filtered Outliers)",
    "text": "1.6 Salary Distribution by Industry (Filtered Outliers)\n\nQ1 = job_postings['SALARY'].quantile(0.25)\nQ3 = job_postings['SALARY'].quantile(0.75)\nIQR = Q3 - Q1\n\nfiltered_salaries = job_postings[\n    (job_postings['SALARY'] &gt;= Q1 - 1.5*IQR) & \n    (job_postings['SALARY'] &lt;= Q3 + 1.5*IQR)\n]\n\nfig = px.box(filtered_salaries, x=\"NAICS_2022_6_NAME\", y=\"SALARY\", \n             title=\"Filtered Salary Distribution by Industry\")\nfig.update_layout(width=1200, height=800, xaxis_tickangle=45)\nfig.update_layout(**plotly_layout)\nfig.show()\n\n                            \n                                            \n\n\nThe box plot above provides a cleaned and focused view of salary distributions across different industries, with extreme outliers removed to highlight more meaningful central trends.\n\nHigh variation across industries: Some industries display a narrow salary band, suggesting standardized roles (e.g., Retail or Administrative sectors), while others—especially in tech, consulting, and finance—show wider spreads, indicating diverse job levels and pay scales.\nTechnology and data-driven sectors (e.g., Computer Systems Design, Custom Software Development) tend to cluster toward the higher end of the salary spectrum, reflecting the premium placed on digital skills, AI, and advanced analytics.\nHealthcare and scientific industries also show strong mid-to-upper ranges, hinting at specialized roles that demand advanced education or certifications.\nIn contrast, industries like Warehousing, Food Services, and Retail generally reflect lower median salaries, consistent with roles requiring less formal education or technical expertise.\n\nThis visualization emphasizes how industry selection can significantly impact earning potential, even before considering role or experience level. For job seekers or workforce planners, it provides a valuable benchmark when evaluating career paths or advising on industry transitions."
  },
  {
    "objectID": "eda.html#fastest-growing-industries-over-time",
    "href": "eda.html#fastest-growing-industries-over-time",
    "title": "Exploratory Data Analysis",
    "section": "1.7 Fastest-Growing Industries Over Time",
    "text": "1.7 Fastest-Growing Industries Over Time\n\nmonthly_industry = (\n    job_postings.groupby([job_postings['POSTED'].dt.to_period(\"M\"), \"NAICS_2022_6_NAME\"])\n    .size()\n    .reset_index(name='Postings')\n)\nmonthly_industry[\"POSTED\"] = monthly_industry[\"POSTED\"].dt.to_timestamp()\n\ntop_industries = monthly_industry.groupby(\"NAICS_2022_6_NAME\")[\"Postings\"].sum().nlargest(6).index\n\ntop_industries = [industry for industry in top_industries if industry != \"Unclassified Industry\"]\n\nfiltered_growth = monthly_industry[monthly_industry[\"NAICS_2022_6_NAME\"].isin(top_industries)]\n\nfig = px.line(filtered_growth, x=\"POSTED\", y=\"Postings\", color=\"NAICS_2022_6_NAME\",\n              title=\"Top 5 Industries by Job Postings Over Time (Excluding Unclassified)\")\nfig.update_layout(**plotly_layout)\nfig.show()\n\n                            \n                                            \n\n\nThis line plot presents job posting trends across the top five industries (excluding unclassified roles), offering a clearer picture of sector-specific hiring momentum over the past several months.\n\nEmployment Placement Agencies show the most significant increase in job postings, suggesting a surge in demand for staffing services. This could reflect broader labor market activity, such as rising contract work, workforce mobility, or seasonal hiring cycles.\nAdministrative Management and Consulting Services maintain consistently high levels of postings, highlighting the ongoing demand for business strategy, operations, and project management talent. The slight upward trend may align with businesses seeking advisory support during periods of uncertainty or transformation.\nComputer Systems Design Services and Custom Computer Programming Services demonstrate steady hiring activity, reinforcing the continued need for tech infrastructure, custom software development, and IT support roles across industries.\nCommercial Banking, while slightly more volatile, remains a key hiring industry. This might reflect fluctuations in financial service needs, regulatory adjustments, or regional economic conditions.\n\nOverall, the chart illustrates that technology, consulting, staffing, and finance remain dominant hiring sectors — with tech-related industries showing stable demand and staffing services accelerating most rapidly. These insights are valuable for job seekers targeting high-opportunity industries, and for workforce planners aiming to align talent strategies with real-time market shifts."
  },
  {
    "objectID": "eda.html#salary-trends-over-time-for-top-5-occupations",
    "href": "eda.html#salary-trends-over-time-for-top-5-occupations",
    "title": "Exploratory Data Analysis",
    "section": "1.8 Salary Trends Over Time for Top 5 Occupations",
    "text": "1.8 Salary Trends Over Time for Top 5 Occupations\n\njob_postings['POSTED'] = pd.to_datetime(job_postings['POSTED'])\ntop_occ = job_postings['LOT_V6_OCCUPATION_NAME'].value_counts().head(5).index\n\nfiltered_jobs = job_postings[job_postings['LOT_V6_OCCUPATION_NAME'].isin(top_occ)]\nfiltered_jobs['Month'] = filtered_jobs['POSTED'].dt.to_period(\"M\").dt.to_timestamp()\n\nsalary_trend = (\n    filtered_jobs.groupby(['Month', 'LOT_V6_OCCUPATION_NAME'])['SALARY']\n    .mean().reset_index()\n)\n\nfig = px.line(salary_trend, \n              x=\"Month\", \n              y=\"SALARY\", \n              color=\"LOT_V6_OCCUPATION_NAME\",\n              title=\"Average Salary Trends Over Time for Top 5 Occupations\")\nfig.update_layout(**plotly_layout)\nfig.show()\n\n/tmp/ipykernel_5852/532501271.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            \n                                            \n\n\nThe line chart illustrates average salary trends over time for the top five most frequently posted occupations. A few meaningful patterns emerge:\n\nComputer Systems Engineer / Architect consistently ranks as the highest-paid occupation, maintaining an average salary around or above $150,000. This reflects the strong demand for highly skilled professionals in systems architecture, a field that supports infrastructure in both legacy enterprises and cloud-native environments.\nData / Data Mining Analysts and Business Intelligence Analysts both show stable and competitive salaries in the range of ~$120,000–$130,000. These roles are closely tied to data-driven decision-making, reflecting how AI and analytics continue to shape business strategy and operations.\nClinical Analysts / Clinical Documentation Specialists demonstrate slightly lower salary levels but remain relatively consistent, indicating steady demand in the healthcare and life sciences sectors—often associated with electronic health records, compliance, and process optimization.\nBusiness / Management Analysts show moderate but stable pay, aligning with generalist consulting and strategic support functions. While their salaries are slightly below the technical roles, they still remain above the $100,000 mark.\n\nOverall, this plot reinforces the idea that technical and analytical occupations—especially those connected to data, engineering, and system-level design—continue to command premium salaries in the job market. Notably, salary stability across all five roles suggests that these are high-value, high-demand positions, resilient to short-term economic shifts."
  },
  {
    "objectID": "eda.html#average-salary-by-employment-type",
    "href": "eda.html#average-salary-by-employment-type",
    "title": "Exploratory Data Analysis",
    "section": "1.9 Average Salary by Employment Type",
    "text": "1.9 Average Salary by Employment Type\n\navg_salary_by_type = (\n    job_postings.groupby(\"EMPLOYMENT_TYPE_NAME\")[\"SALARY\"]\n    .mean()\n    .sort_values(ascending=False)\n    .reset_index()\n)\n\nimport plotly.express as px\n\nfig = px.bar(avg_salary_by_type, \n             x=\"EMPLOYMENT_TYPE_NAME\", \n             y=\"SALARY\",\n             title=\"Average Salary by Employment Type\",\n             labels={\"SALARY\": \"Average Salary ($)\", \"EMPLOYMENT_TYPE_NAME\": \"Employment Type\"},\n             text=\"SALARY\")\n\nfig.update_layout(yaxis_tickprefix=\"$\", height=500)\nfig.update_layout(**plotly_layout)\nfig.show()\n\n                            \n                                            \n\n\nThis bar chart compares the average salaries across different employment types, revealing key patterns in compensation based on job structure:\n\nFull-time roles (&gt;32 hours) lead with the highest average salary at approximately $117,324, which aligns with expectations — these positions often come with more responsibilities, benefits, and long-term career opportunities.\nPart-time / full-time hybrid roles earn slightly less on average (~$104,379), potentially due to inconsistent hours or project-based employment models that offer flexibility but not always the highest compensation.\nPart-time roles (≤32 hours) average just below $102,000, a surprisingly competitive figure. This could reflect specialized part-time positions (e.g., consultants or contract professionals) that still command high hourly rates despite reduced hours.\n\nNotably, the relatively narrow gap between employment types suggests that skills and job function may have a stronger influence on salary than hours alone. High-paying part-time and hybrid roles could indicate a shift toward flexible, high-skill labor markets, where experienced professionals negotiate premium pay for reduced workloads."
  },
  {
    "objectID": "palmer_penguins.html",
    "href": "palmer_penguins.html",
    "title": "Job Market Analysis 2024",
    "section": "",
    "text": "import os\nimport sys\nfrom pyspark.sql import SparkSession\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nspark = SparkSession.builder \\\n    .appName(\"palmerpenguins\") \\\n    .config(\"spark.driver.host\", \"localhost\") \\\n    .getOrCreate()\n\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n\n# Load the CSV file into a Spark DataFrame\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"penguins.csv\")\n\n# 3. Register the DataFrame as a temporary SQL table\ndf.createOrReplaceTempView(\"palmerpenguins\")\ndf.printSchema()\n\nroot\n |-- species: string (nullable = true)\n |-- island: string (nullable = true)\n |-- bill_length_mm: string (nullable = true)\n |-- bill_depth_mm: string (nullable = true)\n |-- flipper_length_mm: string (nullable = true)\n |-- body_mass_g: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- year: integer (nullable = true)\n\n\n\n\ndf = df.dropna(\"any\")\ndf.show(5)\n\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|\n| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|\n| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|\n| Adelie|Torgersen|            NA|           NA|               NA|         NA|    NA|2007|\n| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\nonly showing top 5 rows\n\n\n\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\ncolumn_list = [\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]\n\nfor column_name in column_list:\n    df = df.withColumn(column_name, col(column_name).cast(DoubleType()))\n\ndf.printSchema()\n\nroot\n |-- species: string (nullable = true)\n |-- island: string (nullable = true)\n |-- bill_length_mm: double (nullable = true)\n |-- bill_depth_mm: double (nullable = true)\n |-- flipper_length_mm: double (nullable = true)\n |-- body_mass_g: double (nullable = true)\n |-- sex: string (nullable = true)\n |-- year: integer (nullable = true)\n\n\n\n\nimport pandas as pd\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npdf = df.toPandas()\nfor colname in [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]:\n    fig = px.histogram(pdf, x=colname, color=\"species\", barmode=\"overlay\", marginal=\"box\", nbins=60)\n    fig.update_layout(title=f\"{colname} Distribution by Species\", template=\"simple_white\")\n    fig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport pandas as pd\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npdf = df.toPandas()\n\n# Clean axis label titles\nlabel_map = {\n    \"bill_length_mm\": \"Bill Length (mm)\",\n    \"bill_depth_mm\": \"Bill Depth (mm)\",\n    \"flipper_length_mm\": \"Flipper Length (mm)\",\n    \"body_mass_g\": \"Body Mass (g)\"\n}\n\n# Create 2x2 subplot grid\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[f\"{label} by Species\" for label in label_map.values()],\n    horizontal_spacing=0.1,\n    vertical_spacing=0.15\n)\n# Column pairs and subplot position\ncolumns = list(label_map.keys())\npositions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n\n\n# Add histograms to each subplot\nfor i, (col, (row, col_num)) in enumerate(zip(columns, positions)):\n    for species in pdf[\"species\"].unique():\n        subset = pdf[pdf[\"species\"] == species]\n        fig.add_trace(\n            go.Histogram(\n                x=subset[col],\n                name=species,\n                nbinsx=60,\n                opacity=0.5,\n                showlegend=True if i == 0 else False  # Only show legend once\n            ),\n            row=row, col=col_num\n        )\n\n    fig.update_xaxes(title_text=f\"&lt;b&gt;{label_map[col]}&lt;/b&gt;\", row=row, col=col_num)\n    fig.update_yaxes(title_text=\"&lt;b&gt;Count&lt;/b&gt;\", row=row, col=col_num)\n\nfig.update_layout(\n    title_text=\"Penguin Measurements Distribution by Species\",\n    height=800,\n    template=\"simple_white\",\n    barmode='overlay'\n)\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport pandas as pd\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npdf = df.toPandas()\n# Clean axis label titles\nlabel_map = {\n    \"bill_length_mm\": \"Bill Length (mm)\",\n    \"bill_depth_mm\": \"Bill Depth (mm)\",\n    \"flipper_length_mm\": \"Flipper Length (mm)\",\n    \"body_mass_g\": \"Body Mass (g)\"\n}\n\n# Create 2x2 subplot grid\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[f\"{label} by Species\" for label in label_map.values()],\n    horizontal_spacing=0.1,\n    vertical_spacing=0.15\n)\n# Column pairs and subplot position\ncolumns = list(label_map.keys())\npositions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n\n\n# Add histograms to each subplot\nfor i, (col, (row, col_num)) in enumerate(zip(columns, positions)):\n    for species in pdf[\"species\"].unique():\n        subset = pdf[pdf[\"species\"] == species]\n        fig.add_trace(\n            go.Histogram(\n                x=subset[col],\n                name=species,\n                nbinsx=60,\n                opacity=0.5,\n                histnorm='probability density',\n                showlegend=True if i == 0 else False  # Only show legend once\n            ),\n            row=row, col=col_num\n        )\n\n    fig.update_xaxes(title_text=f\"&lt;b&gt;{label_map[col]}&lt;/b&gt;\", row=row, col=col_num)\n    fig.update_yaxes(title_text=\"&lt;b&gt;Probability Density&lt;/b&gt;\", row=row, col=col_num)\n\nfig.update_layout(\n    title_text=\"Penguin Measurements Distribution by Species\",\n    height=800,\n    template=\"simple_white\",\n    barmode='overlay'\n)\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\npdf_renamed  = pdf.rename(columns=label_map)\n\n# Feature pairs to plot in 2×2 grid (customized for nice visual contrast)\npairs = [\n    (\"bill_length_mm\", \"bill_depth_mm\"),\n    (\"flipper_length_mm\", \"body_mass_g\"),\n    (\"bill_length_mm\", \"flipper_length_mm\"),\n    (\"bill_depth_mm\", \"body_mass_g\"),\n]\n\n# Prepare the figure layout\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[\n        f\"{label_map[x]} vs. {label_map[y]}\" for x, y in pairs\n    ],\n    horizontal_spacing=0.08,\n    vertical_spacing=0.12\n)\n\n# Plot each scatter\nrow_col = [(1, 1), (1, 2), (2, 1), (2, 2)]\n\nspecies_list = pdf[\"species\"].unique()\ncolors = px.colors.qualitative.Plotly\n\nfor idx, ((x, y), (r, c)) in enumerate(zip(pairs, row_col)):\n    for i, species in enumerate(species_list):\n        species_data = pdf[pdf[\"species\"] == species]\n        fig.add_trace(\n            go.Scatter(\n                x=species_data[x],\n                y=species_data[y],\n                mode=\"markers\",\n                name=species if (r, c) == (1, 1) else None,  # Avoid legend repetition\n                marker=dict(color=colors[i], size=6),\n                showlegend=(r, c) == (1, 1)\n            ),\n            row=r, col=c\n        )\n    # Update axis titles\n    fig.update_xaxes(title_text=f\"&lt;b&gt;{label_map[x]}&lt;/b&gt;\", row=r, col=c)\n    fig.update_yaxes(title_text=f\"&lt;b&gt;{label_map[y]}&lt;/b&gt;\", row=r, col=c)\n\n# Layout formatting\nfig.update_layout(\n    height=900,\n    template=\"simple_white\",\n    title=\"&lt;b&gt;Pairwise Feature Scatterplots of Penguin Data&lt;/b&gt;\",\n    title_x=0.5\n)\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfrom pyspark.sql.functions import col, trim, lower\n\ndf_clustering = df.select(\n    col('species'),\n    col(\"bill_length_mm\"),\n    col(\"bill_depth_mm\"),\n    col(\"flipper_length_mm\"),\n    col(\"body_mass_g\")\n)\ndf_clustering = df_clustering.na.drop(\"any\")\ndf_clustering.show(5)\n\n+-------+--------------+-------------+-----------------+-----------+\n|species|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|\n+-------+--------------+-------------+-----------------+-----------+\n| Adelie|          39.1|         18.7|            181.0|     3750.0|\n| Adelie|          39.5|         17.4|            186.0|     3800.0|\n| Adelie|          40.3|         18.0|            195.0|     3250.0|\n| Adelie|          36.7|         19.3|            193.0|     3450.0|\n| Adelie|          39.3|         20.6|            190.0|     3650.0|\n+-------+--------------+-------------+-----------------+-----------+\nonly showing top 5 rows\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.types import DoubleType\n\nprint(\"Remaining rows:\", df_clustering.count())\nfor col_name in df_clustering.columns:\n    nulls = df_clustering.filter(col(col_name).isNull()).count()\n    print(f\"{col_name} nulls: {nulls}\")\n\nRemaining rows: 342\nspecies nulls: 0\nbill_length_mm nulls: 0\nbill_depth_mm nulls: 0\nflipper_length_mm nulls: 0\nbody_mass_g nulls: 0\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n    outputCol=\"features\"\n)\ndata = assembler.transform(df_clustering).select(\"features\", \"species\")\ndata.show(5)\n\n+--------------------+-------+\n|            features|species|\n+--------------------+-------+\n|[39.1,18.7,181.0,...| Adelie|\n|[39.5,17.4,186.0,...| Adelie|\n|[40.3,18.0,195.0,...| Adelie|\n|[36.7,19.3,193.0,...| Adelie|\n|[39.3,20.6,190.0,...| Adelie|\n+--------------------+-------+\nonly showing top 5 rows\n\n\n\n\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nimport plotly.graph_objects as go\n\nscores = []\nks = list(range(2, 9))\n\nfor k in ks:\n    kmeans = KMeans().setK(k).setSeed(42).setFeaturesCol(\"features\")\n    model = kmeans.fit(data)\n    transformed = model.transform(data)\n\n    evaluator = ClusteringEvaluator(\n        featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\n    score = evaluator.evaluate(transformed)\n    scores.append(score)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=ks, y=scores, mode='lines+markers', name='Silhouette Score'))\nfig.update_layout(title=\"Silhouette Score vs. k\", \n                  xaxis_title=\"k\",\n                  yaxis_title=\"Score\",\n                  template=\"simple_white\",\n                  width=900)\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfrom pyspark.ml.feature import PCA\n\npca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\npca_model = pca.fit(data)\nprojected = pca_model.transform(data)\npdf_proj = projected.toPandas()\n\npdf_proj[\"x\"] = pdf_proj[\"pcaFeatures\"].apply(lambda v: float(v[0]))\npdf_proj[\"y\"] = pdf_proj[\"pcaFeatures\"].apply(lambda v: float(v[1]))\n\nfig = px.scatter(pdf_proj, x=\"x\", y=\"y\", color=\"species\", title=\"PCA Projection Colored by Species\")\nfig.update_layout(template=\"simple_white\",\n                  width=900, \n                  height=700,\n                  )\nfig.update_xaxes(title_text=\"&lt;b&gt;Principal Component 1&lt;/b&gt;\")\nfig.update_yaxes(title_text=\"&lt;b&gt;Principal Component 2&lt;/b&gt;\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfrom pyspark.ml.clustering import GaussianMixture\n\ngmm = GaussianMixture(k=3, seed=42, featuresCol=\"features\")\ngmm_model = gmm.fit(data)\ngmm_pred = gmm_model.transform(data)\ngmm_pred.show(5)\n\n+--------------------+-------+--------------------+----------+\n|            features|species|         probability|prediction|\n+--------------------+-------+--------------------+----------+\n|[39.1,18.7,181.0,...| Adelie|[0.99999999816896...|         0|\n|[39.5,17.4,186.0,...| Adelie|[0.99999999900226...|         0|\n|[40.3,18.0,195.0,...| Adelie|[0.99999999872651...|         0|\n|[36.7,19.3,193.0,...| Adelie|[0.99999999749832...|         0|\n|[39.3,20.6,190.0,...| Adelie|[0.99999999145061...|         0|\n+--------------------+-------+--------------------+----------+\nonly showing top 5 rows\n\n\n\n\n# Project GMM predictions\nprojected_gmm = pca_model.transform(gmm_pred)\npdf_gmm = projected_gmm.select(\"pcaFeatures\", \"prediction\").toPandas()\npdf_gmm[\"x\"] = pdf_gmm[\"pcaFeatures\"].apply(lambda v: float(v[0]))\npdf_gmm[\"y\"] = pdf_gmm[\"pcaFeatures\"].apply(lambda v: float(v[1]))\n\n\n\nfig = px.scatter(pdf_gmm, x=\"x\", y=\"y\", color=pdf_gmm[\"prediction\"].astype(str),\n                 title=\"GMM Clustering on PCA-Projected Features\")\nfig.update_layout(template=\"simple_white\",\n                  width=900, \n                  height=700,)\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.mixture import GaussianMixture\n\n# Step 1: Fit GMM on 2D PCA-projected data\nX_proj = pdf_gmm[[\"x\", \"y\"]].values\ngmm_model = GaussianMixture(n_components=len(pdf_gmm[\"prediction\"].unique()), covariance_type='full', random_state=0)\ngmm_model.fit(X_proj)\n\n# Step 2: Create a mesh grid over x and y\nx = np.linspace(X_proj[:, 0].min() - 1, X_proj[:, 0].max() + 1, 300)\ny = np.linspace(X_proj[:, 1].min() - 1, X_proj[:, 1].max() + 1, 300)\nxx, yy = np.meshgrid(x, y)\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Step 3: Predict soft probabilities (posterior) over the mesh\nprobs = gmm_model.predict_proba(grid)\ncluster_map = np.argmax(probs, axis=1).reshape(xx.shape)\n\n# Step 4: Plot prediction area + data points\nfig = go.Figure()\n\n# Add contour for soft cluster regions\nfig.add_trace(go.Contour(\n    x=x, y=y, z=cluster_map,\n    showscale=False,\n    colorscale=\"Viridis\",\n    opacity=0.3,\n    contours=dict(\n        coloring=\"heatmap\",\n        showlines=False\n    )\n))\n\n# Add data points\nfor label in sorted(pdf_gmm[\"prediction\"].unique()):\n    cluster_data = pdf_gmm[pdf_gmm[\"prediction\"] == label]\n    fig.add_trace(go.Scatter(\n        x=cluster_data[\"x\"], y=cluster_data[\"y\"],\n        mode=\"markers\",\n        name=f\"Cluster {label}\",\n        marker=dict(size=5),\n        hoverinfo=\"text\",\n        text=[f\"Cluster: {label}\" for _ in range(len(cluster_data))]\n    ))\n\n# Update layout\nfig.update_layout(\n    title=\"GMM Predicted Clusters with Region Overlay (PCA Projection)\",\n    xaxis_title=\"PCA 1\",\n    yaxis_title=\"PCA 2\",\n    template=\"simple_white\",\n    height=600,\n    width=800\n)\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site aims to cover a profound investigation related to Trends in Salaries and Compensations in AI vs non AI careers"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Salary & Compensation Trends in AI vs. Non-AI Careers",
    "section": "",
    "text": "Recent research has highlighted a growing divergence in salary trends between artificial intelligence (AI)-focused careers and more traditional data science roles. Zhu ((2024)) found that professionals specializing in AI-related fields, such as machine learning engineers and AI researchers, consistently command higher salaries than their non-AI counterparts, including data analysts and general data scientists. This difference in compensation reflects the increasing demand for AI expertise as industries integrate automation, deep learning, and predictive analytics into their operations. While AI roles require specialized skills in areas such as neural networks and natural language processing, traditional data science positions often focus more on business intelligence, statistical analysis, and data visualization, which though even that is valuable it does not see the same salary premiums.\nOther studies reinforce this trend, showing how company size and industry specialization further impact salary structures. Chen, Song, and Lam ((2024)) analyzed U.S. salary trends from 2020 to 2023, reporting that salaries in AI-driven roles have shown steady increases, particularly within mid-to-large tech companies investing in AI innovation. In contrast, non-AI data science roles, such as data analysts, have experienced slower growth, and some projections indicate potential stagnation or slight salary declines in 2024. Similarly, Quan and Raheem ((2023)) found that professionals with expertise in AI, cloud computing, and big data technologies earn higher salaries than those with more generalist skills. Their findings suggest that as AI adoption expands across industries, the wage gap between AI and non-AI roles may continue to grow, emphasizing the importance of specialized technical expertise for long-term career advancement in data science."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom collections import Counter\nimport json\nimport ast\n\n\n1 Team-based Skill Dataframe\nWith our chosen IT career path as Business Analysts, we identified our current skills relevant to the role and assessed proficiency levels using a numerical scale from 1 to 5:\n1 = Beginner\n2 = Basic Knowledge\n3 = Intermediate\n4 = Advanced\n5 = Expert\nThe following heatmap visualized our team strengths and gaps.\n\njob_postings = pd.read_csv(\"lightcast_job_postings.csv\", low_memory = False)\n\n\nteam_skills_data = {\n    \"Name\": [\"Furong\", \"Marco\"],\n    \"R\": [3, 3],\n    \"Python\": [3, 4],\n    \"SQL\": [2, 3],\n    \"Microsoft Excel\": [5, 5],\n    \"Data Visulization\": [4, 4],\n    \"Amazon Web Services\": [2, 2],\n    \"Risk Analytics\": [3, 3],\n    \"Data Mining\": [3, 3]\n}\n\ndf_team_skills = pd.DataFrame(team_skills_data)\ndf_team_skills.set_index(\"Name\", inplace = True)\n\n\nplt.figure(figsize = (8, 6))\nheatmap = sns.heatmap(df_team_skills, annot = True, cmap = \"YlGnBu\", \n                    linewidths = 0.5, fmt = \".1f\", vmin = 1, vmax = 5)\n\ncbar = heatmap.collections[0].colorbar\ncbar.set_ticks([1, 2, 3, 4, 5])\ncbar.set_ticklabels(['Beginner', 'Basic', 'Intermediate', 'Advanced', 'Expert'])\n\nplt.title(\"Team Skill Levels Heatmap\", fontsize = 16)\nplt.ylabel(\"Average Proficiency (1-5)\", fontsize = 12)\nplt.xlabel(\"Skills\", fontsize = 12)\nplt.xticks(rotation = 40, ha = 'right')\nplt.yticks(rotation = 0, ha = 'right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2 Team Skills vs. Industry Requirements\nTo compare our team’s skills to industry requirements, we identified the most in-demand skills from IT job postings. We focused on the industry group Computing Infrastructure Providers, Data Processing, Web Hosting, and Related Services, as it closely aligns with our chosen career path.\nThe bar plot below illustrates the top 10 skills most in demand within IT job postings, providing insights into industry expectations.\n\nit_jobs = job_postings[job_postings['NAICS_2022_6'] == 518210]\n\n\nall_skills = []\n\ndef parse_skills(skills_str):\n    try:\n        if pd.isna(skills_str):\n            return []\n        try:\n            return json.loads(skills_str)\n        except:\n            return ast.literal_eval(skills_str)\n    except:\n        print(f\"Warning: Could not parse skills: {skills_str}\")\n        return []\n\nfor skills_str in it_jobs['SKILLS_NAME'].dropna():\n    skills_list = parse_skills(skills_str)\n    all_skills.extend(skills_list)\n\nskill_counter = Counter(all_skills)\ntop_10_skills = skill_counter.most_common(10)\n\ntop_skills_df = pd.DataFrame(top_10_skills, columns = ['Skill', 'Count'])\n\n\ntotal_postings = len(it_jobs)\ntop_skills_df['Percentage'] = (top_skills_df['Count'] / total_postings * 100).round(1)\ntop_skills_df = top_skills_df.sort_values('Count', ascending = False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x = 'Count', \n    y = 'Skill', \n    data = top_skills_df,\n    hue = 'Skill',\n    palette = \"Blues_r\"\n)\nplt.title(\"Top 10 Skills Required in IT Job Postings\", fontsize = 16)\nplt.xlabel(\"Number of Job Postings\", fontsize = 12)\nplt.ylabel(\"Skills\", fontsize = 12)\nplt.xlim(0, 500) \n\nfor i, row in enumerate(top_skills_df.itertuples()):\n    plt.text(\n        row.Count + 5, \n        i, \n        f\"{row.Count} ({row.Percentage}%)\",\n        va='center'\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3 Team Skills Improvement Plan\nThe bar plot of the top 10 most in-demand skills from IT job postings underscores the significance of core technical skills such as Computer Science (35.8%), Data Analysis (32.4%), SQL (30.2%), and Python (24.9%). These skills highlight the industry’s emphasis on data-driven decision-making and coding expertise as fundamental requirements.\nWhen compared to our team’s current skill set, it is evident that we should prioritize developing knowledge in Data Science and SQL, which are highly demanded by the industry. Furthermore, while our team assessment did not initially account for Communication, the plot shows it as the most sought-after skill in IT job postings. This serves as a crucial reminder of the importance of teamwork, collaboration, and effective interpersonal interactions in IT careers. Technical expertise alone is insufficient; the ability to articulate ideas and communicate solutions is critical.\nThe data also reveals that employers value a combination of technical proficiency, soft skills, and operational knowledge. To address skill gaps within our team, we can adopt a collaborative approach:\n\nKnowledge-sharing and mentorship: Team members skilled in communication can guide others in developing interpersonal abilities, while those with expertise in Python or SQL can lead technical workshops.\n\nSpecialization by strengths: Individuals strong in communication can take on tasks such as presentations and stakeholder engagement, whereas those skilled in technology can focus on back-end technical work.\n\nThis strategy not only bridges skill gaps but also optimizes team collaboration, positioning us as more competitive in the IT job market."
  }
]