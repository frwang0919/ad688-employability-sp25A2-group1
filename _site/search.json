[
  {
    "objectID": "Salary Trends.html",
    "href": "Salary Trends.html",
    "title": "Salary & Compensation Trends in AI vs. Non-AI Careers",
    "section": "",
    "text": "Recent research has highlighted a growing divergence in salary trends between artificial intelligence (AI)-focused careers and more traditional data science roles. Zhu ((2024)) found that professionals specializing in AI-related fields, such as machine learning engineers and AI researchers, consistently command higher salaries than their non-AI counterparts, including data analysts and general data scientists. This difference in compensation reflects the increasing demand for AI expertise as industries integrate automation, deep learning, and predictive analytics into their operations. While AI roles require specialized skills in areas such as neural networks and natural language processing, traditional data science positions often focus more on business intelligence, statistical analysis, and data visualization, which though even that is valuable it does not see the same salary premiums.\nOther studies reinforce this trend, showing how company size and industry specialization further impact salary structures. Chen, Song, and Lam ((2024)) analyzed U.S. salary trends from 2020 to 2023, reporting that salaries in AI-driven roles have shown steady increases, particularly within mid-to-large tech companies investing in AI innovation. In contrast, non-AI data science roles, such as data analysts, have experienced slower growth, and some projections indicate potential stagnation or slight salary declines in 2024. Similarly, Quan and Raheem ((2023)) found that professionals with expertise in AI, cloud computing, and big data technologies earn higher salaries than those with more generalist skills. Their findings suggest that as AI adoption expands across industries, the wage gap between AI and non-AI roles may continue to grow, emphasizing the importance of specialized technical expertise for long-term career advancement in data science."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site aims to cover a profound investigation related to Trends in Salaries and Compensations in AI vs non AI careers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Analysis 2024",
    "section": "",
    "text": "The global job market is undergoing a significant transformation due to various factors, including the rapid adoption of artificial intelligence (AI), evolving work models such as remote work, and shifts in industry-specific wage structures. For job seekers, understanding these dynamics is crucial for making informed decisions about career paths, salary expectations, and work environment preferences. This research aims to analyze salary trends for AI and non-AI careers, as well as the impact of remote work, regional differences, and specific industry on salaries.\n\n\n\nAs the economy adapts to AI technologies, many industries are experiencing shifts in job structures and salary scales. This topic is important because job seekers need to be aware of the changing landscape to make career decisions that align with future trends and personal goals. These years, the rise of AI and automation is creating new job roles while displacing others. Moreover, remote work, which played an important role during the pandemic, has become a permanent feature for many industries. However, questions remain about how compensation for remote roles compares to in-office positions and how it varies across regions and industries. Based on typical trends on the job market, this analysis will explore several key findings:\n\nHow do salaries differ across AI vs. non-AI careers?\nWhat regions offer the highest-paying jobs in AI-related and traditional careers?\nAre remote jobs better paying than in-office roles?\nWhat industries saw the biggest wage growth in 2024?"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Job Market Analysis 2024",
    "section": "",
    "text": "The global job market is undergoing a significant transformation due to various factors, including the rapid adoption of artificial intelligence (AI), evolving work models such as remote work, and shifts in industry-specific wage structures. For job seekers, understanding these dynamics is crucial for making informed decisions about career paths, salary expectations, and work environment preferences. This research aims to analyze salary trends for AI and non-AI careers, as well as the impact of remote work, regional differences, and specific industry on salaries."
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Job Market Analysis 2024",
    "section": "",
    "text": "As the economy adapts to AI technologies, many industries are experiencing shifts in job structures and salary scales. This topic is important because job seekers need to be aware of the changing landscape to make career decisions that align with future trends and personal goals. These years, the rise of AI and automation is creating new job roles while displacing others. Moreover, remote work, which played an important role during the pandemic, has become a permanent feature for many industries. However, questions remain about how compensation for remote roles compares to in-office positions and how it varies across regions and industries. Based on typical trends on the job market, this analysis will explore several key findings:\n\nHow do salaries differ across AI vs. non-AI careers?\nWhat regions offer the highest-paying jobs in AI-related and traditional careers?\nAre remote jobs better paying than in-office roles?\nWhat industries saw the biggest wage growth in 2024?"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\n\n\n\n\n\nSpecifically, we will remove:\n1. Older NAICS and SOC codes (e.g., NAICS2, SOC_2). The North American Industry Classification System (NAICS) and Standard Occupational Classification (SOC) systems undergo periodic updates. Retaining only NAICS_2022_6 and SOC_2021_4 ensures we use the most recent classification standards. Moreover, older codes are redundant and may lead to inconsistencies in trend analysis.\n2. Tracking data and URLs (e.g., ID, DUPLICATES). These columns related to data collection timestamps, unique identifiers, or internal system references, which do not contribute to meaningful insights about the job market. Similarly, URLs are not necessary for our analysis as they do not provide any additional value or context but add unnecessary complexity to the dataset.\n\n\n\nThere are some reasons for this. Firstly, keeping only the latest industry and occupation classifications ensures our analysis reflects the most recent classification standards and avoid confusion and inconsistencies in classification. Additionally, reducing unnecessary columns speeds up data processing and enhances readability. This is particularly important when working with large datasets, as it minimizes the risk of errors and improves the efficiency of our analysis. Finally, it helps to focus on the most relevant information, allowing for clearer insights and conclusions regarding job market trends.\n\n\n\nBy removing outdated and irrelevant columns, we achieve: - More accurate job market trends, focusing on meaningful variables. - Easier interpretation without clutter from redundant or technical fields. - Faster analysis and visualization, improving overall efficiency.\n\njob_postings = pd.read_csv('lightcast_job_postings.csv')\n\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\", \"ACTIVE_URLS\", \"TITLE\", \"COMPANY\",\n    \"MSA\", \"STATE\", \"COUNTY\", \"CITY\", \"COUNTY_OUTGOING\", \"COUNTY_INCOMING\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    \"ONET\", \"ONET_2019\", \"CIP2\", \"CIP4\", \"CIP6\", \"MODELED_DURATION\", \"MODELED_EXPIRED\",\n    \"CERTIFICATIONS\", \"COMMON_SKILLS\", \"SPECIALIZED_SKILLS\", \"SKILLS\", \"SOFTWARE_SKILLS\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_OCCUPATION\", \"LOT_CAREER_AREA\",\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\",\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\", \"NAICS_2022_4\", \"NAICS_2022_4_NAME\",\n    \"NAICS_2022_5\", \"NAICS_2022_5_NAME\", \"NAICS_2022_6\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_5\", \"SOC_5_NAME\", \"SOC_4\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\", \"SOC_2021_4\"\n]\n\njob_postings.drop(columns = columns_to_drop, inplace = True)\n\n\n\n\n\n\n\n\nmsno.heatmap(job_postings)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/matrix.py:260: FutureWarning: Format strings passed to MaskedConstant are ignored, but in future may error or produce different behavior\n  annotation = (\"{:\" + self.fmt + \"}\").format(val)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 msno.heatmap(job_postings)\n      2 plt.title(\"Missing Values Heatmap\")\n      3 plt.show()\n\nFile /opt/anaconda3/lib/python3.11/site-packages/missingno/missingno.py:398, in heatmap(df, filter, n, p, sort, figsize, fontsize, labels, label_rotation, cmap, vmin, vmax, cbar, ax)\n    395 ax0.patch.set_visible(False)\n    397 for text in ax0.texts:\n--&gt; 398     t = float(text.get_text())\n    399     if 0.95 &lt;= t &lt; 1:\n    400         text.set_text('&lt;1')\n\nValueError: could not convert string to float: '--'\n\n\n\n\n\n\n\n\n\n\n\n\nThe SALARY column has a significant number of missing values. To handle this, we replaced the missing values with the median salary for that specific title or industry. This approach is effective because it minimizes the impact of outliers and provides a more accurate representation of the typical salary for each job title.\n\ntitle_median_salary = job_postings.groupby('TITLE_NAME')['SALARY'].median()\nindustry_median_salary = job_postings.groupby('NAICS_2022_6_NAME')['SALARY'].median()\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: title_median_salary[row['TITLE_NAME']]\n    if pd.isna(row['SALARY']) and row['TITLE_NAME'] in title_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: industry_median_salary[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['SALARY']) and row['NAICS_2022_6_NAME'] in industry_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'].fillna(job_postings[\"SALARY\"].median(), inplace = True)\n\n\n\n\nDealing with columns that have more than 50% missing values is crucial for maintaining the integrity of our dataset. Columns with excessive missing data can introduce bias and reduce the reliability of our analysis. Therefore, we removed any columns that exceed this threshold. This ensures that our dataset remains focused on relevant and reliable information, enhancing the quality of our insights.\n\njob_postings.dropna(thresh = len(job_postings) * 0.5, axis = 1, inplace = True)\n\n\n\n\nCategorical fields, such as TITLE_RAW, were filled with “Unknown” for missing values. This approach allows us to retain the integrity of the dataset without introducing bias from arbitrary values. By labeling missing categorical data as “Unknown”, we can still analyze trends without losing valuable information.\n\njob_postings['TITLE_RAW'].fillna(\"Unknown\", inplace = True)\njob_postings['TITLE_CLEAN'].fillna(\"Unknown\", inplace = True)\njob_postings['COMPANY_RAW'].fillna(\"Unknown\", inplace = True)\njob_postings['MSA_NAME'].fillna(\"Unknown\", inplace = True)\njob_postings['MSA_NAME_OUTGOING'].fillna(\"Unknown\", inplace = True)\njob_postings['MSA_NAME_INCOMING'].fillna(\"Unknown\", inplace = True)\n\n\n\n\nFor the EXPIRED variable, we chose to fill the missing values with the maximum date from this column. We assumed that the missing value here is because the post has not expired yet. By using the maximum date, we can effectively handle missing values without introducing bias or skewing the results.\n\njob_postings['POSTED'] = pd.to_datetime(job_postings['POSTED'])\njob_postings['EXPIRED'] = pd.to_datetime(job_postings['EXPIRED'])\n\n\nmax_expired_date = job_postings['EXPIRED'].max()\njob_postings['EXPIRED'] = job_postings['EXPIRED'].fillna(max_expired_date)\n\n\n\n\nFor the MIN_YEARS_EXPERIENCE variable, we chose to fill the missing values with the median MIN_YEARS_EXPERIENCE for a specific title or industry, similar to how we did with the SALARY variable. This can minimize the impact of outliers and provides a more accurate representation of the typical years of experience required for each job title.\n\ntitle_median_exp = job_postings.groupby('TITLE_NAME')['MIN_YEARS_EXPERIENCE'].median()\nindustry_median_exp = job_postings.groupby('NAICS_2022_6_NAME')['MIN_YEARS_EXPERIENCE'].median()\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: title_median_exp[row['TITLE_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['TITLE_NAME'] in title_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: industry_median_exp[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['NAICS_2022_6_NAME'] in industry_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'].fillna(job_postings[\"MIN_YEARS_EXPERIENCE\"].median(), inplace = True)\n\nDURATION variable is also a numerical field, but it has a different approach. We will fill the missing values with the difference between the POSTED and EXPIRED, which calculates the actual time span based on the available dates.\n\ndef impute_duration(cols):\n    posted = cols[0]\n    expired = cols[1]\n    duration = cols[2]\n\n    if pd.isnull(duration):\n        return expired - posted\n    else: \n        return duration\n\n\njob_postings['DURATION'] = job_postings[['POSTED', 'EXPIRED', 'DURATION']].apply(impute_duration, axis = 1)\n\n/var/folders/5p/y9jm3th974l51d8bv_79_fv40000gn/T/ipykernel_53777/2113401602.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  posted = cols[0]\n/var/folders/5p/y9jm3th974l51d8bv_79_fv40000gn/T/ipykernel_53777/2113401602.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  expired = cols[1]\n/var/folders/5p/y9jm3th974l51d8bv_79_fv40000gn/T/ipykernel_53777/2113401602.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  duration = cols[2]\n\n\n\n\n\n\n\n\n\n\njob_postings = job_postings.drop_duplicates(subset=[\"TITLE_NAME\", \"COMPANY_NAME\", \"LOCATION\", \"POSTED\"], keep = \"first\")"
  },
  {
    "objectID": "data_analysis.html#dropping-unnecessary-columns",
    "href": "data_analysis.html#dropping-unnecessary-columns",
    "title": "Data Analysis",
    "section": "",
    "text": "Specifically, we will remove:\n1. Older NAICS and SOC codes (e.g., NAICS2, SOC_2). The North American Industry Classification System (NAICS) and Standard Occupational Classification (SOC) systems undergo periodic updates. Retaining only NAICS_2022_6 and SOC_2021_4 ensures we use the most recent classification standards. Moreover, older codes are redundant and may lead to inconsistencies in trend analysis.\n2. Tracking data and URLs (e.g., ID, DUPLICATES). These columns related to data collection timestamps, unique identifiers, or internal system references, which do not contribute to meaningful insights about the job market. Similarly, URLs are not necessary for our analysis as they do not provide any additional value or context but add unnecessary complexity to the dataset.\n\n\n\nThere are some reasons for this. Firstly, keeping only the latest industry and occupation classifications ensures our analysis reflects the most recent classification standards and avoid confusion and inconsistencies in classification. Additionally, reducing unnecessary columns speeds up data processing and enhances readability. This is particularly important when working with large datasets, as it minimizes the risk of errors and improves the efficiency of our analysis. Finally, it helps to focus on the most relevant information, allowing for clearer insights and conclusions regarding job market trends.\n\n\n\nBy removing outdated and irrelevant columns, we achieve: - More accurate job market trends, focusing on meaningful variables. - Easier interpretation without clutter from redundant or technical fields. - Faster analysis and visualization, improving overall efficiency.\n\njob_postings = pd.read_csv('lightcast_job_postings.csv')\n\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\", \"ACTIVE_URLS\", \"TITLE\", \"COMPANY\",\n    \"MSA\", \"STATE\", \"COUNTY\", \"CITY\", \"COUNTY_OUTGOING\", \"COUNTY_INCOMING\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    \"ONET\", \"ONET_2019\", \"CIP2\", \"CIP4\", \"CIP6\", \"MODELED_DURATION\", \"MODELED_EXPIRED\",\n    \"CERTIFICATIONS\", \"COMMON_SKILLS\", \"SPECIALIZED_SKILLS\", \"SKILLS\", \"SOFTWARE_SKILLS\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_OCCUPATION\", \"LOT_CAREER_AREA\",\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\",\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\", \"NAICS_2022_4\", \"NAICS_2022_4_NAME\",\n    \"NAICS_2022_5\", \"NAICS_2022_5_NAME\", \"NAICS_2022_6\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_5\", \"SOC_5_NAME\", \"SOC_4\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\", \"SOC_2021_4\"\n]\n\njob_postings.drop(columns = columns_to_drop, inplace = True)"
  },
  {
    "objectID": "data_analysis.html#handling-missing-values",
    "href": "data_analysis.html#handling-missing-values",
    "title": "Data Analysis",
    "section": "",
    "text": "msno.heatmap(job_postings)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/matrix.py:260: FutureWarning: Format strings passed to MaskedConstant are ignored, but in future may error or produce different behavior\n  annotation = (\"{:\" + self.fmt + \"}\").format(val)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 msno.heatmap(job_postings)\n      2 plt.title(\"Missing Values Heatmap\")\n      3 plt.show()\n\nFile /opt/anaconda3/lib/python3.11/site-packages/missingno/missingno.py:398, in heatmap(df, filter, n, p, sort, figsize, fontsize, labels, label_rotation, cmap, vmin, vmax, cbar, ax)\n    395 ax0.patch.set_visible(False)\n    397 for text in ax0.texts:\n--&gt; 398     t = float(text.get_text())\n    399     if 0.95 &lt;= t &lt; 1:\n    400         text.set_text('&lt;1')\n\nValueError: could not convert string to float: '--'\n\n\n\n\n\n\n\n\n\n\n\n\nThe SALARY column has a significant number of missing values. To handle this, we replaced the missing values with the median salary for that specific title or industry. This approach is effective because it minimizes the impact of outliers and provides a more accurate representation of the typical salary for each job title.\n\ntitle_median_salary = job_postings.groupby('TITLE_NAME')['SALARY'].median()\nindustry_median_salary = job_postings.groupby('NAICS_2022_6_NAME')['SALARY'].median()\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: title_median_salary[row['TITLE_NAME']]\n    if pd.isna(row['SALARY']) and row['TITLE_NAME'] in title_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'] = job_postings.apply(\n    lambda row: industry_median_salary[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['SALARY']) and row['NAICS_2022_6_NAME'] in industry_median_salary else row['SALARY'], \n    axis=1\n)\n\n\njob_postings['SALARY'].fillna(job_postings[\"SALARY\"].median(), inplace = True)\n\n\n\n\nDealing with columns that have more than 50% missing values is crucial for maintaining the integrity of our dataset. Columns with excessive missing data can introduce bias and reduce the reliability of our analysis. Therefore, we removed any columns that exceed this threshold. This ensures that our dataset remains focused on relevant and reliable information, enhancing the quality of our insights.\n\njob_postings.dropna(thresh = len(job_postings) * 0.5, axis = 1, inplace = True)\n\n\n\n\nCategorical fields, such as TITLE_RAW, were filled with “Unknown” for missing values. This approach allows us to retain the integrity of the dataset without introducing bias from arbitrary values. By labeling missing categorical data as “Unknown”, we can still analyze trends without losing valuable information.\n\njob_postings['TITLE_RAW'].fillna(\"Unknown\", inplace = True)\njob_postings['TITLE_CLEAN'].fillna(\"Unknown\", inplace = True)\njob_postings['COMPANY_RAW'].fillna(\"Unknown\", inplace = True)\njob_postings['MSA_NAME'].fillna(\"Unknown\", inplace = True)\njob_postings['MSA_NAME_OUTGOING'].fillna(\"Unknown\", inplace = True)\njob_postings['MSA_NAME_INCOMING'].fillna(\"Unknown\", inplace = True)\n\n\n\n\nFor the EXPIRED variable, we chose to fill the missing values with the maximum date from this column. We assumed that the missing value here is because the post has not expired yet. By using the maximum date, we can effectively handle missing values without introducing bias or skewing the results.\n\njob_postings['POSTED'] = pd.to_datetime(job_postings['POSTED'])\njob_postings['EXPIRED'] = pd.to_datetime(job_postings['EXPIRED'])\n\n\nmax_expired_date = job_postings['EXPIRED'].max()\njob_postings['EXPIRED'] = job_postings['EXPIRED'].fillna(max_expired_date)\n\n\n\n\nFor the MIN_YEARS_EXPERIENCE variable, we chose to fill the missing values with the median MIN_YEARS_EXPERIENCE for a specific title or industry, similar to how we did with the SALARY variable. This can minimize the impact of outliers and provides a more accurate representation of the typical years of experience required for each job title.\n\ntitle_median_exp = job_postings.groupby('TITLE_NAME')['MIN_YEARS_EXPERIENCE'].median()\nindustry_median_exp = job_postings.groupby('NAICS_2022_6_NAME')['MIN_YEARS_EXPERIENCE'].median()\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: title_median_exp[row['TITLE_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['TITLE_NAME'] in title_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'] = job_postings.apply(\n    lambda row: industry_median_exp[row['NAICS_2022_6_NAME']]\n    if pd.isna(row['MIN_YEARS_EXPERIENCE']) and row['NAICS_2022_6_NAME'] in industry_median_exp else row['MIN_YEARS_EXPERIENCE'], \n    axis=1\n)\n\n\njob_postings['MIN_YEARS_EXPERIENCE'].fillna(job_postings[\"MIN_YEARS_EXPERIENCE\"].median(), inplace = True)\n\nDURATION variable is also a numerical field, but it has a different approach. We will fill the missing values with the difference between the POSTED and EXPIRED, which calculates the actual time span based on the available dates.\n\ndef impute_duration(cols):\n    posted = cols[0]\n    expired = cols[1]\n    duration = cols[2]\n\n    if pd.isnull(duration):\n        return expired - posted\n    else: \n        return duration\n\n\njob_postings['DURATION'] = job_postings[['POSTED', 'EXPIRED', 'DURATION']].apply(impute_duration, axis = 1)\n\n/var/folders/5p/y9jm3th974l51d8bv_79_fv40000gn/T/ipykernel_53777/2113401602.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  posted = cols[0]\n/var/folders/5p/y9jm3th974l51d8bv_79_fv40000gn/T/ipykernel_53777/2113401602.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  expired = cols[1]\n/var/folders/5p/y9jm3th974l51d8bv_79_fv40000gn/T/ipykernel_53777/2113401602.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  duration = cols[2]"
  },
  {
    "objectID": "data_analysis.html#removing-duplicate-job-postings",
    "href": "data_analysis.html#removing-duplicate-job-postings",
    "title": "Data Analysis",
    "section": "",
    "text": "job_postings = job_postings.drop_duplicates(subset=[\"TITLE_NAME\", \"COMPANY_NAME\", \"LOCATION\", \"POSTED\"], keep = \"first\")"
  },
  {
    "objectID": "data_analysis.html#job-postings-by-industry",
    "href": "data_analysis.html#job-postings-by-industry",
    "title": "Data Analysis",
    "section": "2.1 Job Postings by Industry",
    "text": "2.1 Job Postings by Industry\n\nfig = px.bar(job_postings[\"NAICS_2022_6_NAME\"].value_counts(), title=\"Job Postings by Industry\")\nfig.show()\n\n                                                \n\n\nThe bar chart illustrates the distribution of job postings across various industries, with a notably high concentration in the “Unclassified Industry” category. This suggests that a significant portion of job postings may lack detailed industry classification, which could impact sector-specific analysis. Among the classified industries, sectors such as Software Publishing, Real Estate Services, and Semiconductor Manufacturing appear to have the highest job postings, indicating strong demand in these fields."
  },
  {
    "objectID": "data_analysis.html#salary-distribution-by-industry",
    "href": "data_analysis.html#salary-distribution-by-industry",
    "title": "Data Analysis",
    "section": "2.2 Salary Distribution by Industry",
    "text": "2.2 Salary Distribution by Industry\n\nfig = px.box(job_postings, x=\"NAICS_2022_6_NAME\", y=\"SALARY\", title=\"Salary Distribution by Industry\")\n\nfig.update_layout(width=1200, height=1000)\n\nfig.show()\n\n                                                \n\n\nThe box plot provides a clearer view of salary distributions across industries, highlighting variations in median salaries and outliers. Most industries exhibit salary concentrations below $200K, with some sectors showing significantly higher outliers above $300K-$500K, suggesting high-paying roles in specialized fields. The dense clustering of salaries at the lower end indicates that a majority of job postings fall within a more constrained range, while a few industries display wider salary spreads, potentially reflecting executive-level or highly specialized positions. Further filtering by industry type or salary range could enhance interpretability."
  },
  {
    "objectID": "data_analysis.html#remote-vs.-on-site-jobs",
    "href": "data_analysis.html#remote-vs.-on-site-jobs",
    "title": "Data Analysis",
    "section": "2.3 Remote vs. On-Site Jobs",
    "text": "2.3 Remote vs. On-Site Jobs\n\nfig = px.pie(job_postings, names=\"REMOTE_TYPE_NAME\", title=\"Remote vs. On-Site Jobs\")\nfig.show()\n\n                                                \n\n\n\nfiltered_job_postings = job_postings[job_postings[\"REMOTE_TYPE_NAME\"] != \"[None]\"]\n\nfig = px.pie(filtered_job_postings, names=\"REMOTE_TYPE_NAME\", title=\"Remote vs. On-Site Jobs (Excluding None)\")\nfig.show()\n\n                                                \n\n\nThe initial pie chart shows that 78.3% of job postings have no specified remote work type, making it difficult to analyze remote work trends accurately. After filtering out these unspecified postings, the revised pie chart provides a clearer view of the distribution of remote job opportunities. Among classified job postings, 78.4% are fully remote, while 14.4% are hybrid, and 7.29% require on-site presence. This suggests that remote work remains the dominant option among classified job postings, with hybrid and on-site roles making up a smaller but notable portion of the market."
  }
]